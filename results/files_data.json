{".DS_Store": null, ".gitignore": "venv\n.env\n/__pycache__\n__pycache__/\n", "all_results/.DS_Store": null, "all_results/1/basic_details.txt": "Repository Details:\n  Description: None\n  Stars: 0\n  Forks: 0\n  Open Issues: 0\n  Watchers: 0\n\nBranches:\n  main\n", "all_results/1/dependencies.txt": "Based on the code provided, here are the libraries and frameworks required:\n\n**Required Libraries:**\n\n1. **OpenCV (cv2)**: For image processing, computer vision tasks, and video capture.\n2. **NumPy**: For efficient numerical computations and data storage.\n3. **Pandas**: Not used explicitly in this code snippet, but may be required for data manipulation or analysis later on.\n\n**Optional Libraries:**\n\n1. **Pygame**: An alternative to OpenCV for display and game development.\n2. **Pyglet**: Another alternative to Pygame for creating games with a more modern API.\n\n**Other Tools and Resources:**\n\n1. **Python Interpreter**: The code is written in Python, so you'll need to have the Python interpreter installed on your system.\n2. **Video Capture Device**: The `cv2.VideoCapture` class requires a video capture device (e.g., webcam) to capture video frames.\n\n**Frameworks and APIs:**\n\nNone explicitly mentioned in this code snippet. However, if you were to expand upon this project or create additional features, you might consider using frameworks like:\n\n1. **Kivy**: A cross-platform Python library for building multi-touch applications.\n2. **PyQt** or **PySide**: Qt-based Python libraries for building GUI applications.\n\nKeep in mind that the specific libraries and frameworks required may vary depending on your project's requirements and goals.", "all_results/1/file_types.json": "{\"py\": 87.5, \"md\": 12.5}", "all_results/1/files_data.json": "{\n  \"Ball.py\": \"import cv2 # type: ignore\\r\\nimport random\\r\\n\\r\\nfrom utils.constants import SPEED_INCREMENT,BALL_RADIUS,BALL_VEL,BALL_COLOR\\r\\n\\r\\nclass Ball:\\r\\n    def __init__(self, x, y):\\r\\n        self.x = self.original_x = x\\r\\n        self.y = self.original_y = y\\r\\n        self.radius = BALL_RADIUS\\r\\n        self.vel = BALL_VEL\\r\\n        self.color = BALL_COLOR\\r\\n        self.x_vel = BALL_VEL\\r\\n        self.y_vel = -1*BALL_VEL\\r\\n\\r\\n    def draw(self, frame):\\r\\n        cv2.circle(frame, (int(self.x), int(self.y)),\\r\\n                   self.radius,self.color, -1)\\r\\n\\r\\n    def move(self,frame):\\r\\n        self.x += self.x_vel\\r\\n        self.y += self.y_vel\\r\\n\\r\\n        self.draw(frame)\\r\\n    \\r\\n    def increase_speed(self):\\r\\n        self.x_vel *= SPEED_INCREMENT\\r\\n        self.y_vel *= SPEED_INCREMENT\\r\\n\\r\\n    def reset(self):\\r\\n        self.x = self.original_x\\r\\n        self.y = self.original_y\\r\\n\\r\\n        # Randmoize direction at every reset\\r\\n        choice = random.random()\\r\\n        x_dir = 1 if choice < 0.5 else -1\\r\\n        choice = random.random()\\r\\n        y_dir = 1 if choice < 0.5 else -1\\r\\n\\r\\n        self.x_vel = x_dir * self.vel \\r\\n        self.y_vel = y_dir * self.vel \",\n  \"Paddle.py\": \"import cv2 #type: ignore\\nfrom utils.constants import HEIGHT,PADDLE_HEIGHT,PADDLE_WIDTH\\n\\nclass Paddle:\\n    def __init__(self, x, y,color):\\n        self.x = x\\n        self.y = y\\n        self.width = PADDLE_WIDTH\\n        self.height = PADDLE_HEIGHT\\n        self.color = color\\n\\n    def draw(self, frame):\\n        cv2.rectangle(\\n            frame,\\n            (int(self.x - self.width // 2), int(self.y - self.height // 2)),\\n            (int(self.x + self.width // 2), int(self.y + self.height // 2)),\\n            self.color,\\n            -1,\\n        )\\n\\n    def move(self, x, y):\\n        self.x = x\\n        self.y = y\\n\\n        # Ensure the paddle stays within the frame boundaries\\n        if self.y - self.height//2 <= 0:\\n            self.y = self.height//2\\n        if self.y + self.height//2 >= HEIGHT:\\n            self.y = HEIGHT - self.height//2\\n        \\n\\n    # def reset(self):\\n    #     self.x = self. original_x\\n    #     self.y = self.original_y\",\n  \"README.md\": \"# virtual-air-hockey\",\n  \"Score.py\": \"import cv2 # type: ignore\\nfrom utils.constants import LEFT_OFFSET,RIGHT_OFFSET,WIDTH,MAX_SCORE\\n\\n\\nclass Score:\\n    def __init__(self):\\n        self.player_a = 0\\n        self.player_b = 0\\n        self.winner = \\\"\\\"\\n        pass\\n\\n    def isWinner(self):\\n        if(self.player_a>=MAX_SCORE):\\n            self.winner = \\\"A\\\"\\n            return True\\n        elif(self.player_b>=MAX_SCORE):\\n            self.winner = \\\"B\\\"\\n            return True\\n        else:\\n            return False\\n        \\n    def calculate_score(self, ball):\\n        if ball.x - ball.radius < LEFT_OFFSET:\\n            self.player_b += 1\\n        elif ball.x + ball.radius > RIGHT_OFFSET:\\n            self.player_a += 1\\n        pass\\n\\n    def reset(self):\\n        self.player_a = 0\\n        self.player_b = 0\\n        self.winner = \\\"\\\"\\n\\n    def show(self, ball, frame):\\n        self.calculate_score(ball)\\n        \\n        if(not self.isWinner()):\\n            cv2.putText(\\n                frame,\\n                f\\\"Player A: {self.player_a}\\\",\\n                (WIDTH//7, 30),\\n                cv2.FONT_HERSHEY_SIMPLEX,\\n                1,\\n                (255, 255, 255),\\n                2,\\n            )\\n            cv2.putText(\\n                frame,\\n                f\\\"Player B: { self.player_b}\\\",\\n                (WIDTH*4//7, 30),\\n                cv2.FONT_HERSHEY_SIMPLEX,\\n                1,\\n                (255, 255, 255),\\n                2,\\n            )\\n        else:\\n            cv2.putText(\\n                frame,\\n                f\\\"Winner is Player: {self.winner}!!!\\\",\\n                (WIDTH*2//7, 30),\\n                cv2.FONT_HERSHEY_SIMPLEX,\\n                1,\\n                (0, 255, 0),\\n                2,\\n            )\\n\\n            cv2.imshow(\\\"Hand Gesture Slider\\\", frame)\\n            cv2.waitKey(5000)\\n            self.reset()\\n            \\n\\n    pass\",\n  \"collision.py\": \"from utils.constants import HEIGHT, LEFT_OFFSET, RIGHT_OFFSET\\n\\n\\ndef handle_collision(ball, left_paddle, right_paddle, frame):\\n    # Collision with side edges\\n    if ball.x - ball.radius < LEFT_OFFSET or ball.x + ball.radius > RIGHT_OFFSET:\\n        ball.reset()\\n\\n    # Collision with top edge\\n    if ball.y - ball.radius <= 0:\\n        ball.y_vel = -ball.y_vel\\n\\n    # Collision with bottom edge\\n    if ball.y + ball.radius >= HEIGHT:\\n        ball.y_vel = -ball.y_vel\\n\\n    # Collision with the right slider\\n    if ball.x + ball.radius >= right_paddle.x - right_paddle.width // 2 and (\\n        ball.y + ball.radius >= right_paddle.y - right_paddle.height // 2 \\n        and ball.y - ball.radius <= right_paddle.y + right_paddle.height // 2\\n    ):\\n        ball.x_vel = -ball.x_vel\\n\\n        ball.increase_speed()\\n\\n    # Collision with left slider\\n    if ball.x - ball.radius <= left_paddle.x + left_paddle.width // 2 and (\\n        ball.y + ball.radius >= left_paddle.y - left_paddle.height // 2\\n        and ball.y - ball.radius <= left_paddle.y + left_paddle.height // 2\\n    ):\\n        ball.x_vel = -ball.x_vel\\n\\n        ball.increase_speed()\\n\\n    ball.move(frame)\\n\",\n  \"constants.py\": \"# Window size\\r\\nWIDTH, HEIGHT = 700, 500\\r\\n\\r\\n# Ball Variables\\r\\nBALL_RADIUS = 15\\r\\nBALL_COLOR = (255,255,255)\\r\\nBALL_VEL = 2\\r\\n\\r\\n# PADDLE VARIABLES\\r\\nPADDLE_WIDTH, PADDLE_HEIGHT = 20, 100\\r\\n\\r\\n# PADDING\\r\\nLEFT_OFFSET = PADDLE_WIDTH\\r\\nRIGHT_OFFSET = WIDTH - LEFT_OFFSET\\r\\n\\r\\n# speed increment factor\\r\\nSPEED_INCREMENT = 1.25\\r\\n\\r\\n# Score to min\\r\\nMAX_SCORE = 5\",\n  \"hand_detection.py\": \"import cv2 # type: ignore\\nimport numpy as np # type: ignore\\n\\ndef empty(a):\\n    pass\\n\\nclass HandDetection():\\n    def __init__(self):\\n        pass\\n\\n    def create_trackbars(self):\\n        cv2.namedWindow('Trackbars')\\n        cv2.resizeWindow('Trackbars', 500, 300)\\n        cv2.createTrackbar('HueMin', 'Trackbars', 0, 179, empty)\\n        cv2.createTrackbar('HueMax', 'Trackbars', 179, 179, empty)\\n        cv2.createTrackbar('SatMin', 'Trackbars', 0, 255, empty)\\n        cv2.createTrackbar('SatMax', 'Trackbars', 255, 255, empty)\\n        cv2.createTrackbar('ValMin', 'Trackbars', 0, 255, empty)\\n        cv2.createTrackbar('ValMax', 'Trackbars', 60, 255, empty)\\n    \\n    def create_mask(self, img):\\n        imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\\n        hue_min = cv2.getTrackbarPos('HueMin', 'Trackbars')\\n        hue_max = cv2.getTrackbarPos('HueMax', 'Trackbars')\\n        sat_min = cv2.getTrackbarPos('SatMin', 'Trackbars')\\n        sat_max = cv2.getTrackbarPos('SatMax', 'Trackbars')\\n        val_min = cv2.getTrackbarPos('ValMin', 'Trackbars')\\n        val_max = cv2.getTrackbarPos('ValMax', 'Trackbars')\\n        lower = np.array([hue_min, sat_min, val_min])\\n        upper = np.array([hue_max, sat_max, val_max])\\n        mask = cv2.inRange(imgHSV, lower, upper)\\n        return mask\\n    \\n    def threshold(self, mask):\\n        _, thresh_img = cv2.threshold(mask, 127, 255, cv2.THRESH_OTSU)\\n        return thresh_img\\n    \\n    def find_contours(self, thresh_img):\\n        contours, _ = cv2.findContours(thresh_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\\n        return contours\\n    \\n    def two_largest_contours(self, contours):\\n        if len(contours) < 2:\\n            return contours  # Return all contours if fewer than 2 exist\\n        sorted_contours = sorted(contours, key=lambda x: cv2.contourArea(x), reverse=True)[:2]\\n        approx_contours = []\\n        for cnt in sorted_contours:\\n            epsilon = 0.005 * cv2.arcLength(cnt, True)\\n            approx = cv2.approxPolyDP(cnt, epsilon, True)\\n            approx_contours.append(approx)\\n        return approx_contours\\n\\n    def clean_image(self, mask):\\n        img_eroded = cv2.erode(mask, (3, 3), iterations=1)\\n        img_dilated = cv2.dilate(img_eroded, (3, 3), iterations=1)\\n        return img_dilated\\n\\n    def centroid(self, contour):\\n        if len(contour) == 0:\\n            return (-1, -1)\\n        M = cv2.moments(contour)\\n        try:\\n            x = int(M['m10'] / M['m00'])\\n            y = int(M['m01'] / M['m00'])\\n        except ZeroDivisionError:\\n            return (-1, -1)\\n        return (x, y)\\n    \\n    def get_centroid(self,frame):\\n        mask = self.create_mask(frame)\\n        clean_mask = self.clean_image(mask)\\n        thresh_img = self.threshold(clean_mask)\\n        contours = self.find_contours(thresh_img)\\n        largest_contours = self.two_largest_contours(contours)\\n        centroids = []\\n        for contour in largest_contours:\\n            cx, cy = self.centroid(contour)\\n            centroids.append((cx, cy))\\n            cv2.drawContours(frame, [contour], -1, (0, 255, 0), 2)\\n            cv2.circle(frame, (cx, cy), 5, (0, 0, 255), -1)\\n        return centroids\\n        \\n\\n################# DRIVER CODE ###################\\n\\n# hd = HandDetection()\\n# hd.create_trackbars()\\n\\n# cap = cv2.VideoCapture(0)\\n\\n# while True:\\n#     ret, frame = cap.read()\\n#     if not ret:\\n#         break\\n\\n#     mask = hd.create_mask(frame)\\n#     clean_mask = hd.clean_image(mask)\\n#     thresh_img = hd.threshold(clean_mask)\\n#     contours = hd.find_contours(thresh_img)\\n#     largest_contours = hd.two_largest_contours(contours)\\n\\n#     for contour in largest_contours:\\n#         cx, cy = hd.centroid(contour)\\n#         cv2.drawContours(frame, [contour], -1, (0, 255, 0), 2)\\n#         cv2.circle(frame, (cx, cy), 5, (255, 0, 0), -1)\\n\\n#     cv2.imshow(\\\"Frame\\\", frame)\\n#     if cv2.waitKey(1) & 0xFF == ord('q'):\\n#         break\\n\\n# cap.release()\\n# cv2.destroyAllWindows()\\n\",\n  \"main.py\": \"import cv2 # type: ignore\\r\\nfrom utils.hand_detection import HandDetection\\r\\nfrom utils.Ball import Ball\\r\\nfrom utils.Paddle import Paddle\\r\\nfrom utils.collision import handle_collision\\r\\nfrom utils.constants import WIDTH, HEIGHT, LEFT_OFFSET, RIGHT_OFFSET\\r\\nfrom utils.Score import Score\\r\\n\\r\\n# Initialize video capture\\r\\nvid = cv2.VideoCapture(0)\\r\\n\\r\\n# Create an instance of HandDetection\\r\\nhand_detection = HandDetection()\\r\\nhand_detection.create_trackbars()\\r\\n\\r\\n# Create an instance of Score\\r\\nscore = Score()\\r\\n\\r\\n\\r\\ndef main():\\r\\n    left_paddle = Paddle(LEFT_OFFSET, HEIGHT // 2, (255, 0, 0))\\r\\n    right_paddle = Paddle(RIGHT_OFFSET, HEIGHT // 2, (0, 255, 0))\\r\\n    ball = Ball(WIDTH // 2, HEIGHT // 2)\\r\\n\\r\\n    while vid.isOpened():\\r\\n        ret, frame = vid.read()\\r\\n        if not ret:\\r\\n            break\\r\\n\\r\\n        # Resize to defined size\\r\\n        frame = cv2.resize(frame, (WIDTH, HEIGHT))\\r\\n\\r\\n        # Flip the frame for\\r\\n        frame = cv2.flip(frame, 1)\\r\\n\\r\\n        # Get Centroids of hands/color\\r\\n        centroids = hand_detection.get_centroid(frame)\\r\\n\\r\\n        # Assign centroids to paddles\\r\\n        if len(centroids) == 1:\\r\\n            left_paddle.move(left_paddle.x, centroids[0][1])\\r\\n        elif len(centroids) == 2:\\r\\n            # Sort by x-coordinate\\r\\n            centroids = sorted(centroids, key=lambda c: c[0])\\r\\n            left_paddle.move(left_paddle.x, centroids[0][1])\\r\\n            right_paddle.move(right_paddle.x, centroids[1][1])\\r\\n        \\r\\n        # Draw paddles\\r\\n        left_paddle.draw(frame)\\r\\n        right_paddle.draw(frame)\\r\\n\\r\\n        # Start move ball\\r\\n        ball.move(frame)\\r\\n\\r\\n        # Display score on frame\\r\\n        score.show(ball, frame)\\r\\n\\r\\n        # Handle collision between ball and paddles\\r\\n        handle_collision(ball, left_paddle, right_paddle, frame)\\r\\n\\r\\n        cv2.imshow(\\\"Hand Gesture Slider\\\", frame)\\r\\n\\r\\n        # Exit\\r\\n        key = cv2.waitKey(10)\\r\\n        if key == ord(\\\"q\\\"):\\r\\n            break\\r\\n\\r\\n    # Release the video capture and close all OpenCV windows\\r\\n    vid.release()\\r\\n    cv2.destroyAllWindows()\\r\\n\\r\\n\\r\\nif __name__ == \\\"__main__\\\":\\r\\n    main()\\r\\n\"\n}\n", "all_results/1/final_summaries.json": "{\n  \"Ball.py\": \"The Ball class simulates a bouncing ball with customizable speed, radius, and color, including methods for drawing, moving, increasing speed, and resetting its position and direction.\",\n  \"Paddle.py\": \"A Paddle class is defined, with attributes for position, color, and size, and methods to draw and move the paddle within frame boundaries.\",\n  \"README.md\": \"I'm happy to help you with your request, but it seems like you forgot to provide the actual content of the README.md file. Could you please paste the contents? I'll be happy to assist you in summarizing it in 30 words.\",\n  \"Score.py\": \"Score tracker for a hand gesture game with two players, displaying scores and declaring winner.\",\n  \"collision.py\": \"Collision detection implemented with checks for side edges, top edge, paddle collisions, and speed increase upon collision. Ball is reset or moved accordingly in each scenario.\",\n  \"constants.py\": \"Constants for a game include window size (700x500), ball variables (radius 15, color white, velocity 2), paddle dimensions and offsets, score limit (5) and speed increment factor (1.25).\",\n  \"hand_detection.py\": \"This Python script is for hand detection using OpenCV. It creates a GUI with trackbars to adjust the hue, saturation, and value ranges of an image. The script then applies these settings to detect hands in live video feed from a webcam.\",\n  \"main.py\": \"This Python script uses OpenCV to capture video from a webcam, detect hand gestures controlling left and right paddles, score points, and handle collisions with the ball.\"\n}\n", "all_results/1/full_project.txt": "This is a Python script that uses OpenCV to detect hand gestures in real-time and control a paddle on the screen. Here's a high-level overview of the code:\n\n1. The script initializes a video capture object using OpenCV, which reads frames from the default camera.\n2. It creates an instance of the `HandDetection` class, which is responsible for detecting hand centroids in each frame.\n3. In the main loop, it reads frames from the video capture object and applies various transformations to the frames:\n\t* Resizes the frames to the desired size (defined by `WIDTH` and `HEIGHT` constants).\n\t* Flips the frames horizontally using `cv2.flip`.\n4. It gets the centroids of hands/color in each frame using the `get_centroid` method of the `HandDetection` instance.\n5. Based on the number of centroids detected, it moves one or two paddles (left and right) to the corresponding x-coordinates and y-coordinates of the centroids.\n6. It draws the paddles on the frames using the `draw` method of the paddle instances.\n7. It starts moving a ball on the screen by calling the `move` method of the `Ball` instance.\n8. It displays the score on the frame using the `show` method of the `Score` instance.\n9. It handles collisions between the ball and paddles by calling the `handle_collision` function from the `collision` module.\n10. Finally, it releases the video capture object and closes all OpenCV windows when the program exits.\n\nThe script uses various modules and classes, including:\n\n* `utils.hand_detection`: The `HandDetection` class is responsible for detecting hand centroids in each frame.\n* `utils.Ball`: The `Ball` class represents a ball on the screen, with methods to move and show it.\n* `utils.Paddle`: The `Paddle` class represents a paddle on the screen, with methods to move and draw it.\n* `utils.collision`: The `handle_collision` function handles collisions between the ball and paddles.\n* `utils.constants`: This module defines various constants used throughout the script, including `WIDTH`, `HEIGHT`, `LEFT_OFFSET`, and others.\n\nOverall, this script uses computer vision techniques to detect hand gestures in real-time and control a paddle on the screen.", "all_results/1/tree_struct.txt": "|-- Ball.py\n|-- Paddle.py\n|-- README.md\n|-- Score.py\n|-- collision.py\n|-- constants.py\n|-- hand_detection.py\n|-- main.py", "all_results/2/basic_details.txt": "Repository Details:\n  Description: DDoS Prevention @SIH event\n  Stars: 1\n  Forks: 2\n  Open Issues: 0\n  Watchers: 1\n\nBranches:\n  BlackList\n  devansh-hotfix\n  main\n", "all_results/2/dependencies.txt": "The provided C++ code project appears to utilize several libraries and frameworks. Based on the files and directories present in the repository, here are some of the notable ones:\n\n**C++ Libraries:**\n\n1. **Boost**: Boost is a comprehensive C++ library that provides various utility functions, containers, algorithms, and more. It's likely used for tasks such as logging, networking, and string manipulation.\n2. **OpenSSL**: OpenSSL is a popular cryptographic library used for secure communication and encryption. It might be utilized for tasks like SSL/TLS protocol implementation or cryptographic hashing.\n3. **Poco**: Poco is a C++ libraries and frameworks for building network-enabled applications. It provides a comprehensive set of libraries for networking, databases, XML parsing, and more.\n\n**C++ Frameworks:**\n\n1. **Qt**: Qt is a popular cross-platform application development framework that provides a comprehensive set of libraries and tools for building GUI applications, networks, and more.\n2. **Eigen**: Eigen is a high-performance linear algebra library used for matrix operations, numerical computations, and other mathematical tasks.\n\n**Other Dependencies:**\n\n1. **CMake**: CMake is a cross-platform build system generator that provides a way to define build configurations, dependencies, and compilation options in a platform-independent manner.\n2. **Make**: Make is a classic Unix-based build tool used for compiling and linking source code files into executable binaries.\n\n**Other Tools:**\n\n1. **GNOME Terminal**: GNOME Terminal is a terminal emulator application used to run the firewall, C analyzer, and Rust analyzer binaries in separate windows.\n2. **xterm**: xterm is another terminal emulator application that might be used on some systems instead of GNOME Terminal.\n\nHere's an example of how you could list these libraries and frameworks as dependencies for your project:\n\n```bash\n# Required libraries\nboost\nopenssl\npoco\n\n# Required frameworks\nqt\neigen\n\n# Other dependencies\ncmake\nmake\n\n# Optional tools\ngnome-terminal\nxterm\n```\n\nNote that some of these dependencies might not be strictly required, and their usage might vary depending on the specific requirements of your project.", "all_results/2/file_types.json": "{\n  \"gitignore\": 10.344827586206897,\n  \"json\": 24.137931034482758,\n  \"Dockerfile\": 3.4482758620689653,\n  \"c\": 3.4482758620689653,\n  \"lock\": 3.4482758620689653,\n  \"toml\": 3.4482758620689653,\n  \"rs\": 3.4482758620689653,\n  \"html\": 3.4482758620689653,\n  \"js\": 6.896551724137931,\n  \"css\": 3.4482758620689653,\n  \"sh\": 13.793103448275861,\n  \"ts\": 3.4482758620689653,\n  \"csv\": 6.896551724137931,\n  \"yaml\": 3.4482758620689653,\n  \"cpp\": 3.4482758620689653,\n  \"md\": 3.4482758620689653\n}\n", "all_results/2/final_summaries.json": "{\n  \".gitignore\": \"Files to ignore: compiled object files (.o), main source code, build directory (target and dist).\",\n  \".powconfig.json\": \"Configuration file detailing hard limits for server connections and requests, including maximum connections per IP address, browser, and specific IP addresses allowed to connect.\",\n  \".vscode/c_cpp_properties.json\": \"\\\"C++ configuration for macOS-clang-arm64, specifying compiler path and include paths, with default standards and intelliSense mode.\\\"\",\n  \".vscode/launch.json\": \"\\\"Launch configuration for C/C++ debugging with Lldb, using /usr/local/bin/lldb and setting working directory to '/Users/jaiyankargupta/PoW-Shield/algos/c'\\\".\",\n  \".vscode/settings.json\": \"Config file for Visual Studio Code C/C++ extension settings, specifying compiler and linker paths, warning flags, and other compiler options for C and C++ projects.\",\n  \"Dockerfile\": \"Docker image based on Ubuntu, installs g++, copies C++ code, compiles and executes it.\",\n  \"algos/c/main.c\": \"Server listens on port 8081, accepts connections, reads user input, checks for \\\"not found\\\" in the user agent, and responds with \\\"safe\\\".\",\n  \"algos/rust/.gitignore\": \"*.o\\n*.a\\n*.so\\n*.exe\",\n  \"algos/rust/Cargo.lock\": \"This is a Cargo.toml file, which is used to manage dependencies for Rust projects. Here's a summary of the contents:\\n\\n**Packages**\\n\\n* `zerocopy`: Version 0.7.35\\n\\t+ Dependencies:\\n\\t\\t- `byteorder`\\n\\t\\t- `zerocopy-derive`\\n* `zerocopy-derive`: Version 0.7.35\\n\\t+ Dependencies:\\n\\t\\t- `proc-macro2`\\n\\t\\t- `quote`\\n\\t\\t- `syn`\\n\\n**System Packages**\\n\\nThe file also lists system packages, which are used to build the Rust compiler and other dependencies. These include:\\n\\n* Windows-specific targets (e.g. `windows_i686_msvc`)\\n* Linux-specific targets (e.g. `linux_x86_64_gnu`)\\n\\n**Other**\\n\\n* The file is likely part of a larger project, with its own Cargo.toml file.\\n* The version numbers of the packages are specified, which allows for version-specific dependencies to be managed.\\n\\nOverall, this file provides detailed information about the dependencies required by a Rust project, including system packages and other dependencies.\",\n  \"algos/rust/Cargo.toml\": \"DDoS-Algos, a Rust package, is a dependency-based framework with versions for Cargo, including csv, serde, smartcore, log, env_logger, serde_json, hyper, tokio, and rand.\",\n  \"algos/rust/output.json\": \"Data summary: IP addresses with their corresponding request details, including browser information, status codes, and response times.\",\n  \"algos/rust/src/main.rs\": \"This is a Rust web server written using the actix-web framework. The server has two main modes of operation:\\n\\n1. **ML Mode**: When the server is run with the `ml` command-line argument, it runs in machine learning mode. In this mode, the server does not respond to HTTP requests and instead focuses on running machine learning models.\\n2. **Server Mode**: When the server is run with the `server` command-line argument, it runs in server mode. In this mode, the server responds to HTTP requests.\\n\\nThe server has several routes defined:\\n\\n* `/`: Serves an index.html file from the static directory.\\n* `/styles.css`: Serves a CSS file from the static directory.\\n* `/script.js`: Serves a JavaScript file from the static directory.\\n* `/pow`: Handles proof-of-work challenges by generating a random question and returning it in JSON format.\\n* `/check`: Handles proof-of-work checks by checking if the provided answer matches the solution to the challenge. If the answer is correct, it returns \\\"valid\\\" in JSON format.\\n\\nThe server also uses several helper functions:\\n\\n* `generate_random_question`: Generates a random question for the proof-of-work challenge.\\n* `check_proof_of_work`: Checks if the provided answer matches the solution to the challenge.\\n\\nOverall, this server appears to be designed for use with a cryptocurrency or blockchain application, where it can handle proof-of-work challenges and other related tasks.\",\n  \"algos/rust/static/index.html\": \"HTML document for displaying DDoS data statistics with a table, containing IP address, timestamp, and request count information.\",\n  \"algos/rust/static/script.js\": \"Data stored in JavaScript file, contains IP addresses as keys with JSON data describing web requests and responses.\",\n  \"algos/rust/static/styles.css\": \"This CSS file sets a basic layout with Arial font, light gray background, and a shadow effect on a container with centered text and alternating row colors.\",\n  \"build.sh\": \"The script checks for GCC and Cargo, compiles C and Rust projects, and handles missing files. It installs dependencies if needed and exits with errors for missing files.\",\n  \"config/.gitignore\": \"Node_modules and temp directories should be ignored by the Git version control system.\",\n  \"config/index.js\": \"The script configures and updates a configuration file (.powconfig.json) using inquirer prompts to set or delete hard limits and the allowed IP.\",\n  \"config/package-lock.json\": \"The provided output appears to be a JSON representation of an npm (Node Package Manager) dependency tree. Here's a breakdown of the data:\\n\\n**Top-level structure**\\n\\n* The root object represents the package dependencies.\\n* Each key-value pair in the root object corresponds to a specific package or library.\\n\\n**Package information**\\n\\n* For each package, the following information is provided:\\n\\t+ `version`: The version number of the package.\\n\\t+ `resolved`: A URL pointing to the package's source code or distribution (e.g., a tarball).\\n\\t+ `integrity`: A SHA-512 hash of the package's contents, used for verification purposes.\\n\\t+ `license` and `(MIT OR CC0-1.0)`: The license under which the package is distributed.\\n\\n**Dependency relationships**\\n\\n* Each package has an array of dependencies, listed as a key-value pair in the root object.\\n* These dependencies are represented as strings, referencing other packages or libraries that need to be installed or linked for the current package to function correctly.\\n\\nSome notable observations from this output:\\n\\n1. **Package versions**: The version numbers suggest that the project is using relatively recent versions of popular Node.js packages (e.g., `undici-types` version 6.19.8, `tmp` version 0.0.33).\\n2. **Dependencies on dependencies**: Some packages have circular dependency references (e.g., `type-fest` depends on `undici-types`, which in turn depends on `type-fest`). These should be resolved by upgrading or downgrading specific package versions.\\n3. **License information**: The license fields provide insight into the project's licensing requirements and compliance.\\n4. **Hash verification**: The integrity hashes ensure that the installed packages have not been tampered with during transmission.\\n\\nTo make sense of this output, you can use tools like `npm ls` (list dependencies) or `npm dedupe` (remove duplicate dependencies). Additionally, reviewing the project's codebase and understanding its dependency graph will help in identifying potential issues or opportunities for improvement.\",\n  \"config/package.json\": \"config/package.json:\\nname: pow-shield\\nversion: 1.0.0\\ndescription: DDoS prevention with multiple algorithms and a proof of work system.\",\n  \"config/sample.ts\": \"Here is a summary of the file in 30 words:\\n\\nThe FirewallManager class reads and writes firewall configuration from/to a JSON file, blocking or unblocking IP addresses while logging entries to a specified log file.\",\n  \"dataGen.sh\": \"The script generates a CSV file with 1 million simulated web traffic records, including random IP addresses, browsers, HTTP methods, and status codes, with every 10000th record simulating a Distributed Denial of Service (DDoS) attempt.\",\n  \"ddos_data.csv\": \"This text appears to be a log file or a record of HTTP requests and responses from a web server or a network device. The format of the log entries suggests that it is in a format similar to the Apache HTTP Server's access log.\\n\\nThe log entries typically consist of 12 fields, separated by spaces:\\n\\n1. Date/Time of the request\\n2. IP address of the client\\n3. Request method (e.g., GET, POST, etc.)\\n4. URL requested\\n5. Status code of the response\\n6. Size of the response body\\n7. Number of bytes sent to the client\\n8. Time taken to send the response\\n9. Referrer URL (if applicable)\\n10. User agent string (if applicable)\\n11. Server software or version\\n12. Requested URI\\n\\nEach entry typically starts with a line that indicates the date and time of the request, followed by the IP address of the client, and then the details of the request.\\n\\nThe log entries also include various error codes and status messages, such as \\\"404 Not Found\\\" or \\\"502 Bad Gateway\\\", which indicate errors or unexpected responses from the server.\\n\\nOverall, this log file appears to provide a record of all requests made to the server, including any errors or exceptions that occurred during processing.\",\n  \"docker-compose.yaml\": \"Docker Compose configuration defines a service named 'firewall' with a built image from the current directory's Dockerfile, mapping port 8080 on the host to 8080 in the container.\",\n  \"firewall/main.cpp\": \"Here is a summary of the provided code:\\n\\n**Overview**\\n\\nThe code implements a simple HTTP server that performs rate limiting and IP blocking for malicious clients. It uses a `RecentCounter` data structure to track requests from client IPs.\\n\\n**Key Features**\\n\\n1. **Rate Limiting**: The server limits the number of requests from each client IP to prevent brute-force attacks.\\n2. **IP Blocking**: If a client IP exceeds the rate limit, it is blocked for a specified period (5 seconds).\\n3. **User-Agent Parsing**: The server parses the User-Agent header and logs requests based on the parsed information.\\n\\n**Implementation**\\n\\n1. The code defines an `unordered_map` called `ipMap` to store recent request counters for each client IP.\\n2. A `RecentCounter` data structure is used to track requests from each client IP, including the number of requests and the last request timestamp.\\n3. When a new connection is established, the server checks if the client IP is in the blacklist or has exceeded the rate limit. If so, it sends an \\\"unsafe\\\" response (HTTP/1.1 200 OK with a specific header) to block the client.\\n4. The server uses the `select` function to wait for incoming data from clients and processes incoming requests based on the User-Agent header.\\n\\n**Notes**\\n\\n* The code uses a simple `select` timeout mechanism, which may not be suitable for production environments.\\n* The blacklist is stored in an `unordered_set`, which has poor performance characteristics compared to other data structures.\\n* The `RecentCounter` data structure is not optimized for performance and may lead to memory leaks or other issues.\\n\\n**Security Concerns**\\n\\n* The code does not perform adequate input validation, making it vulnerable to attacks such as SQL injection or cross-site scripting (XSS).\\n* The \\\"unsafe\\\" response sent to blocked clients can be exploited by attackers.\\n* The blacklist is stored in an `unordered_set`, which may lead to performance issues if the list grows large.\\n\\n**Improvement Suggestions**\\n\\n1. Implement a more efficient data structure for storing blacklists, such as a `std::set` or a database.\\n2. Use a more secure method for sending responses to blocked clients, such as using HTTP/2 or HTTPS.\\n3. Improve input validation and sanitization techniques to prevent attacks.\\n4. Optimize the `RecentCounter` data structure for performance.\\n5. Consider implementing additional security measures, such as IP geolocation or web application firewall (WAF) integration.\",\n  \"log.csv\": \"IP Address: 127.0.0.1; Browser: Safari; Request Count: 17; Time (ms): 1725130988491.\",\n  \"readme.md\": \"PoW-Shield prevents DDoS attacks using multiple algorithms, a proof of work system, and validating requests with Redis integration, pending implementation tasks and manual testing.\",\n  \"run.sh\": \"A Bash script checks for compiled binaries, then runs the firewall, C analyzer, and Rust analyzer in a terminal of choice (gnome-terminal or xterm) if available, otherwise opening it manually.\",\n  \"test.sh\": \"This bash script sends a GET request to a specified URL, optionally with custom headers, using the `curl` command.\"\n}\n", "all_results/2/full_project.txt": "The provided code appears to be a Python script that implements a basic firewall system with three different algorithms (C, Rust, and unknown) to detect and filter incoming network traffic. The script also includes functionality to send HTTP requests using the `curl` command.\n\nHere's a breakdown of the main components:\n\n1. **Firewall System**: The script defines a basic firewall system that can be extended with additional algorithms and features.\n2. **Algorithms**:\n\t* **C Analyzer**: A C-based algorithm that analyzes incoming traffic and filters out suspicious packets.\n\t* **Rust Analyzer**: A Rust-based algorithm that uses proof-of-work to validate requests.\n\t* **Unknown Algorithm**: Currently, the script does not implement this algorithm, but it appears to be a placeholder for future development.\n3. **HTTP Request Sending**: The script includes functionality to send HTTP requests using the `curl` command, which can be used to test the firewall system or simulate incoming traffic.\n4. **Logging and Configuration**: The script logs information about incoming traffic and request counts in a CSV file named `log.csv`. It also includes configuration files for the firewall system (`firewall/main`) and the C Analyzer (`algos/c/main`).\n\nSome potential improvements and suggestions:\n\n*   **Security**: While the script provides a basic framework for building a firewall, it may not be secure enough to protect against all types of attacks. Consider implementing additional security features, such as intrusion detection and prevention.\n*   **Performance**: The script uses the `curl` command to send HTTP requests, which can be slow and resource-intensive. Consider optimizing the script for better performance or using alternative libraries that can handle this task more efficiently.\n*   **User Interface**: The script does not provide a user interface for configuring the firewall system or monitoring traffic in real-time. Consider adding a GUI or CLI-based interface to make it easier to use and manage the firewall.\n*   **Error Handling**: The script does not include robust error handling mechanisms, which can lead to unexpected behavior or crashes if an error occurs. Consider implementing try-except blocks and logging mechanisms to handle errors more effectively.\n\nOverall, the script provides a solid foundation for building a basic firewall system with three different algorithms. However, it requires further development and testing to make it more secure, performant, and user-friendly.", "all_results/2/tree_struct.txt": "|-- .gitignore\n|-- .powconfig.json\n|-- .vscode\n    |-- c_cpp_properties.json\n    |-- launch.json\n    |-- settings.json\n|-- 1_ddos_data.csv\n|-- Dockerfile\n|-- algos\n    |-- c\n        |-- main.c\n    |-- rust\n        |-- .gitignore\n        |-- Cargo.lock\n        |-- Cargo.toml\n        |-- output.json\n        |-- src\n            |-- main.rs\n        |-- static\n            |-- index.html\n            |-- script.js\n            |-- styles.css\n|-- build.sh\n|-- config\n    |-- .gitignore\n    |-- index.js\n    |-- package-lock.json\n    |-- package.json\n    |-- sample.ts\n|-- dataGen.sh\n|-- ddos_data.csv\n|-- docker-compose.yaml\n|-- firewall\n    |-- firewall\n    |-- main.cpp\n|-- log.csv\n|-- readme.md\n|-- run.sh\n|-- test.sh", "get_file_dat_API.py": "import requests\nimport json\nimport os\nfrom fastapi import FastAPI\nfrom dotenv import load_dotenv\n\nrunAPI = False\n\nload_dotenv()\n\ndirectory = \"results\"\nif not os.path.exists(directory):\n    os.mkdir(directory)\n\n\n# GitHub API endpoint\nGITHUB_GRAPHQL_URL = \"https://api.github.com/graphql\"\nMAX_SIZE_IN_KB = 50\n# Your GitHub Personal Access Token\n# GITHUB_TOKEN = os.getenv(\"GIT_TOKEN\")\n#\n\nGITHUB_TOKEN = os.getenv(\"GIT_TOKEN\")\nif not GITHUB_TOKEN:\n    raise ValueError(\"GitHub token not found. Please set the GIT_TOKEN environment variable.\")\n\n\n# Define the GraphQL query\nquery = \"\"\"\nquery ($owner: String!, $repo: String!, $expression: String!) {\n  repository(owner: $owner, name: $repo) {\n    object(expression: $expression) {\n      ... on Tree {\n        entries {\n          name\n          type\n          object {\n            ... on Blob {\n              text\n              byteSize\n            }\n            ... on Tree {\n              entries {\n                name\n                type\n                object {\n                  ... on Blob {\n                    text\n                    byteSize\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\"\"\"\n\nheaders = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n\nnum_of_folders = 0\n\ndef fetch_repo_details(owner, repo):\n    basic = \"\"\n    # Endpoint to fetch basic repo details\n    repo_url = f'https://api.github.com/repos/{owner}/{repo}'\n    repo_response = requests.get(repo_url, headers=headers)\n    if repo_response.ok:\n        repo_data = repo_response.json()\n        stars = repo_data.get('stargazers_count')\n        forks = repo_data.get('forks_count')\n        issues = repo_data.get('open_issues_count')\n        watchers = repo_data.get('watchers_count')\n        description = repo_data.get('description')\n        basic += (\"Repository Details:\") +'\\n'\n        basic += (f\"  Description: {description}\") +'\\n'\n        basic += (f\"  Stars: {stars}\") +'\\n'\n        basic += (f\"  Forks: {forks}\") +'\\n'\n        basic += (f\"  Open Issues: {issues}\") +'\\n'\n        basic += (f\"  Watchers: {watchers}\") + '\\n'\n\n    else:\n        print(\"Failed to fetch repository details\")\n        return None\n\n    # Endpoint to fetch branches\n    branches_url = f'https://api.github.com/repos/{owner}/{repo}/branches'\n    branches_response = requests.get(branches_url, headers=headers)\n    if branches_response.ok:\n        branches_data = branches_response.json()\n        branch_names = [branch.get('name') for branch in branches_data]\n        basic += (\"\\nBranches:\\n\")\n        for name in branch_names:\n            basic += (f\"  {name}\\n\")\n    else:\n        print(\"Failed to fetch branches\")\n\n    return basic\n\n\ndef fetch_repo_contents(owner, repo, expression):\n    \"\"\"\n    Recursively fetches all files in a GitHub repository.\n    \"\"\"\n    # headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n    variables = {\"owner\": owner, \"repo\": repo, \"expression\": expression}\n    response = requests.post(\n        GITHUB_GRAPHQL_URL, json={\"query\": query, \"variables\": variables}, headers=headers\n    )\n\n    if response.status_code != 200:\n        raise Exception(f\"Query failed: {response.status_code}, {response.text}\")\n\n    data = response.json()\n    entries = data[\"data\"][\"repository\"][\"object\"][\"entries\"]\n    files = {}\n\n    for entry in entries:\n        #  and entry[\"object\"][\"byteSize\"] < 1000\n        if entry[\"type\"] == \"blob\" and entry[\"object\"][\"byteSize\"] < MAX_SIZE_IN_KB*1024:\n            # It's a file, get its content\n            files[entry[\"name\"]] = entry.get(\"object\", {}).get(\"text\", None)\n        elif entry[\"type\"] == \"tree\":\n            # It's a directory, recursively fetch its contents\n\n            sub_expression = f\"{expression}{entry['name']}/\"\n            sub_files = fetch_repo_contents(owner, repo, sub_expression)\n            files.update({f\"{entry['name']}/{k}\": v for k, v in sub_files.items()})\n\n    return files\n\n\n\ndef get_repo_tree(owner, repo, branch=\"main\"):\n\n    # Step 1: Get branch details to retrieve tree SHA\n    branch_url = f\"https://api.github.com/repos/{owner}/{repo}/branches/{branch}\"\n    branch_response = requests.get(branch_url, headers=headers)\n    if not branch_response.ok:\n        raise Exception(\"Failed to fetch branch details\")\n    branch_data = branch_response.json()\n    tree_sha = branch_data[\"commit\"][\"commit\"][\"tree\"][\"sha\"]\n\n    # Step 2: Fetch the recursive tree structure\n    tree_url = f\"https://api.github.com/repos/{owner}/{repo}/git/trees/{tree_sha}?recursive=1\"\n    tree_response = requests.get(tree_url, headers=headers)\n    if not tree_response.ok:\n        raise Exception(\"Failed to fetch repository tree\")\n    tree_data = tree_response.json()\n\n    return tree_data\n\n\ndef print_directory_tree(tree_data):\n    \"\"\"\n    Process and print a simple text-based tree structure.\n    \"\"\"\n    # Build a hierarchical dictionary from the flat list of tree entries.\n\n    structure = \"\"\n    tree = {}\n    for item in tree_data.get(\"tree\", []):\n        parts = item[\"path\"].split(\"/\")\n        current = tree\n        for part in parts:\n            current = current.setdefault(part, {})\n    def build_tree(subtree, prefix=\"\"):\n        lines = []\n        for key, nested in sorted(subtree.items()):\n            lines.append(prefix + \"|-- \" + key)\n            lines.extend(build_tree(nested, prefix + \"    \"))\n        return lines\n    structure = build_tree(tree)\n    return \"\\n\".join(structure)\n\n\n# Example usage\nowner = \"Himasnhu-AT\"  # Replace with the repo owner username\nrepo = \"FastSearch\"         # Replace with the repository name\nbranch = \"main\"            # Replace with the branch name\n\nif(runAPI):\n    app = FastAPI()\ntry:\n    all_files = fetch_repo_contents(owner, repo, f\"{branch}:\")\n    # for file_path, content in all_files.items():\n        # print(f\"File: {\n        # file_path}\")\n        # print(f\"Content: {content[:100]}...\")  # Print the first 100 characters of content\n    with open(\"results/files_data.json\", 'w') as f:\n        json.dump(all_files, f)\n    # print(all_files)\n    # with open(\"checking_new.json\", 'w') as f:\n    #     json.dump(all_files, f)\n\n    if(runAPI):\n        app = FastAPI()\n\n        @app.get(\"/get_file_data\")\n        async def get_file_data():\n            return all_files\nexcept Exception as e:\n    print(e)\n\n\ntry:\n    basic_details = fetch_repo_details(owner= owner, repo= repo)\n    if(basic_details is None):\n        print(\"can't fetch details.\")\n    else:\n        with open(\"results/basic_details.txt\", 'w') as f:\n            f.write(basic_details)\n\nexcept Exception as e:\n    print(e)\n\n\ntry:\n    tree_data = get_repo_tree(owner, repo, branch)\n    treee = print_directory_tree(tree_data)\n    with open(\"results/tree_struct.txt\", 'w') as f:\n        f.write(treee)\n\nexcept Exception as e:\n    print(e)\n    print(\"e in tree\")\n\nif __name__ == \"__main__\" and runAPI:\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n", "get_file_types.py": "import json\n\nwith open('results/files_data.json', 'r') as file:\n    data = json.load(file)\n\nfile_types = {}\ntotalFiles = 0;\n\nfor filename, __ in data.items():\n    file_ext = (filename.split(\".\"))[-1]\n    if file_ext not in file_types:\n        file_types[file_ext] = 0\n    file_types[file_ext] += 1;\n    totalFiles+=1\n\nfor types, num in file_types.items():\n    percentage = (num/totalFiles)*100\n    file_types[types] = percentage\n\n\nwith open(\"results/file_types.json\", 'w') as f:\n    json.dump(file_types, f)\n\n# print(file_types)\n", "misc_bin/check_printing.py": "import json\n\n\nwith open('files_data.json', 'r') as file:\n    data = json.load(file)\n\n\nfor _, dat in data.items():\n    print(dat)\n", "misc_bin/checking.py": "import json\nimport requests \nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\n# At the top we can construct the request we want to make \nGITHUB_BASE_URL = 'https://api.github.com' \nrepo = 'noterAI' \nowner = \"Himasnhu-AT\"\nurl = f\"{GITHUB_BASE_URL}/repos/{owner}/{repo}\"\n\ntoken = os.getenv(\"GIT_TOKEN\")\n# token = os.environ.get(\"GET_TOKEN\")\n\nheaders = {\n    \"Authorization\": f\"token {token}\",\n    \"Accept\": \"application/vnd.github.v3+json\"\n}\n\nresponse = requests.get(url, headers=headers)\n\ndef bprint(repo):\n    with open(\"some.json\", 'w') as f:\n        json.dump(repo, f)\n\nif response.status_code == 200:\n    repo_data = response.json()\n    print(f\"Repository Name: {repo_data['name']}\")\n    print(f\"Description: {repo_data['description']}\")\n    print(f\"Stars: {repo_data['stargazers_count']}\")\n    print(f\"Forks: {repo_data['forks_count']}\")\n    # print(repo_data)\n    bprint(repo_data)\nelse:\n    print(f\"Failed to fetch data: {response.status_code}\")", "misc_bin/checking_new.json": "{\n  \".vs/ProjectSettings.json\": \"{\\n  \\\"CurrentProjectSetting\\\": \\\"x64-Release\\\"\\n}\",\n  \".vs/VSWorkspaceState.json\": \"{\\n  \\\"ExpandedNodes\\\": [\\n    \\\"\\\",\\n    \\\"\\\\\\\\problem_sets\\\"\\n  ],\\n  \\\"SelectedNode\\\": \\\"\\\\\\\\problem_sets\\\\\\\\WeirdAlgorithm.cpp\\\",\\n  \\\"PreviewInSolutionExplorer\\\": false\\n}\",\n  \".vscode/c_cpp_properties.json\": \"{\\n    \\\"configurations\\\": [\\n        {\\n            \\\"name\\\": \\\"Win32\\\",\\n            \\\"includePath\\\": [\\n                \\\"${workspaceFolder}/**\\\"\\n            ],\\n            \\\"defines\\\": [\\n                \\\"_DEBUG\\\",\\n                \\\"UNICODE\\\",\\n                \\\"_UNICODE\\\"\\n            ],\\n            \\\"windowsSdkVersion\\\": \\\"10.0.22621.0\\\",\\n            \\\"compilerPath\\\": \\\"C:/MinGW/bin/g++.exe\\\",\\n            \\\"cStandard\\\": \\\"c17\\\",\\n            \\\"cppStandard\\\": \\\"c++20\\\",\\n            \\\"intelliSenseMode\\\": \\\"windows-msvc-x64\\\"\\n        }\\n    ],\\n    \\\"version\\\": 4\\n}\",\n  \"995/1.py\": \"def maximize_difference(n, a, b, i=0, m=0, s=0):\\n    # Base case: no more days to consider\\n    if i >= n:\\n        return m - s\\n\\n    # Option 1: Monocarp skips this day\\n    skip = maximize_difference(n, a, b, i + 1, m, s)\\n    \\n    # Option 2: Monocarp trains on this day\\n    stereocarp_contrib = b[i + 1] if i + 1 < n else 0\\n    train = maximize_difference(n, a, b, i + 2, m + a[i], s + stereocarp_contrib)\\n    \\n    # Return the maximum of the two options\\n    return max(skip, train)\\n\\n# Process multiple test cases\\ndef solve():\\n    t = int(input())  # Number of test cases\\n    results = []\\n    for _ in range(t):\\n        n = int(input())\\n        a = list(map(int, input().split()))\\n        b = list(map(int, input().split()))\\n        # Compute the result for the current test case\\n        results.append(maximize_difference(n, a, b))\\n    \\n    # Print all results\\n    for res in results:\\n        print(res)\\n\\n# Example Input/Output\\n# Uncomment below lines to run locally\\nsolve()\\n\",\n  \"995/4.py\": \"from bisect import bisect_left, bisect_right\\n\\ndef count_interesting_pairs(t, test_cases):\\n    results = []\\n    for case in test_cases:\\n        n, x, y, a = case\\n        S = sum(a)\\n        L, R = S - y, S - x\\n        a.sort()\\n        count = 0\\n        \\n        for i in range(n):\\n            low = L - a[i]\\n            high = R - a[i]\\n            j_left = bisect_left(a, low, i + 1)\\n            j_right = bisect_right(a, high, i + 1)\\n            count += j_right - j_left\\n        \\n        results.append(count)\\n    \\n    return results\\n\\nt = int(input())\\ntest_cases = []\\nfor _ in range(t):\\n    n, x, y = map(int, input().split())\\n    a = list(map(int, input().split()))\\n    test_cases.append((n, x, y, a))\\n\\nresults = count_interesting_pairs(t, test_cases)\\nprint(\\\"\\\\n\\\".join(map(str, results)))\\n\",\n  \"CppProperties.json\": \"{\\n  \\\"configurations\\\": [\\n    {\\n      \\\"inheritEnvironments\\\": [\\n        \\\"msvc_x64\\\"\\n      ],\\n      \\\"name\\\": \\\"x64-Release\\\",\\n      \\\"includePath\\\": [\\n        \\\"${env.INCLUDE}\\\",\\n        \\\"${workspaceRoot}\\\\\\\\**\\\"\\n      ],\\n      \\\"defines\\\": [\\n        \\\"WIN32\\\",\\n        \\\"NDEBUG\\\",\\n        \\\"UNICODE\\\",\\n        \\\"_UNICODE\\\"\\n      ],\\n      \\\"intelliSenseMode\\\": \\\"windows-msvc-x64\\\"\\n    }\\n  ]\\n}\",\n  \"Introduction/chess_queen.cpp\": \"#include <iostream>\\n#include <fstream>\\n#include <sstream>\\n#include <iomanip>\\n#include <string>  \\n#include <vector>\\n#include <list>        \\n#include <set> \\n#include <map>    \\n#include <queue> \\n#include <stack>\\n#include <algorithm>  \\n#include <cmath> \\n#include <ctime> \\n#include <cstdlib>\\n#include <cstring> \\n#include <cctype> \\n#include <cassert>\\n#include <exception>   \\n#include <functional>\\n#include <iterator>\\n#include <limits>  \\n#include <locale>   \\n#include <numeric>  \\n#include <random> \\n#include <stdexcept> \\n#include <typeinfo> \\n#include <utility>\\nusing namespace std;\\nint main()\\n{\\n    \\n    return 0;\\n}\",\n  \"Introduction/coin_piles.cpp\": \"#include <iostream>\\n#include <fstream>\\n#include <sstream>\\n#include <iomanip>\\n#include <string>  \\n#include <vector>\\n#include <list>        \\n#include <set> \\n#include <map>    \\n#include <queue> \\n#include <stack>\\n#include <algorithm>  \\n#include <cmath> \\n#include <ctime> \\n#include <cstdlib>\\n#include <cstring> \\n#include <cctype> \\n#include <cassert>\\n#include <exception>   \\n#include <functional>\\n#include <iterator>\\n#include <limits>  \\n#include <locale>   \\n#include <numeric>  \\n#include <random> \\n#include <stdexcept> \\n#include <typeinfo> \\n#include <utility>\\nusing namespace std;\\nint main()\\n{\\n    int t;\\n    cin >> t;\\n    while(t--){\\n        long a, b;\\n        cin >> a >> b;\\n\\n        if(a == b){\\n            cout << \\\"YES\\\" << endl;\\n            continue;\\n        }\\n        else if(a == b-1 || a-1 == b){\\n            cout << \\\"YES\\\" << endl;\\n        }\\n        else{\\n            cout << \\\"NO\\\" << endl;\\n        }\\n    }\\n    return 0;\\n}\",\n  \"Introduction/digit_queries.cpp\": \"#include <iostream>\\n#include <fstream>\\n#include <sstream>\\n#include <iomanip>\\n#include <string>  \\n#include <vector>\\n#include <list>        \\n#include <set> \\n#include <map>    \\n#include <queue> \\n#include <stack>\\n#include <algorithm>  \\n#include <cmath> \\n#include <ctime> \\n#include <cstdlib>\\n#include <cstring> \\n#include <cctype> \\n#include <cassert>\\n#include <exception>   \\n#include <functional>\\n#include <iterator>\\n#include <limits>  \\n#include <locale>   \\n#include <numeric>  \\n#include <random> \\n#include <stdexcept> \\n#include <typeinfo> \\n#include <utility>\\nusing namespace std;\\nint main()\\n{\\n    \\n    return 0;\\n}\",\n  \"Introduction/gray_code.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\n\\nvoid decToBinary(int n, int maxx)\\n{\\n    // Size of an integer is assumed to be 32 bits\\n    for (int i = maxx; i >= 0; i--) {\\n        int k = n >> i;\\n        if (k & 1)\\n            cout << \\\"1\\\";\\n        else\\n            cout << \\\"0\\\";\\n    }\\n}\\n\\nint main(){\\n    ios::sync_with_stdio(0);\\n    cin.tie(0);\\n    // og num then n>>1 and take XOR for n\\n    // 100110\\n    // 010011\\n\\n    // 110101\\n\\n    int n;\\n    cin >> n;\\n    long long maxx = pow(2, n);\\n    for(int i=0;i < maxx;i++){\\n        long long newnum = (i)^(i>>1);\\n        decToBinary(newnum, n-1);\\n        cout << endl;\\n    }\\n    return 0;\\n}\",\n  \"Introduction/increasing_array.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\nint main()\\n{\\n\\n    int n;\\n    cin >> n;\\n    vector<int> arr(n);\\n    for(int i=0;i < n;i++){\\n        cin >> arr[i];\\n    }\\n\\n    int val = arr[0];\\n    long long ans = 0;\\n    for(int i=1;i < n;i++){\\n        if(arr[i] < val){\\n            ans += val - arr[i];\\n            arr[i] = val;\\n        }\\n        else val = arr[i];\\n    }\\n    cout << ans << endl;\\n   return 0;\\n}\",\n  \"Introduction/missing_number.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\nint main()\\n{\\n    int n;\\n    cin >> n;\\n    unordered_set<int> s;\\n    for(int i=0;i < n-1;i++){\\n        int val;\\n        cin >> val;\\n        s.insert(val);\\n    }\\n \\n    for(int i=1;i <= n;i++){\\n        if(s.find(i) == s.end()){\\n            cout << i;\\n            return 0;\\n        }\\n    }\\n    return 1;\\n}\\n\",\n  \"Introduction/number_spiral.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\nint main(){\\n    ios::sync_with_stdio(0);\\n    cin.tie(0);\\n    \\n    return 0;\\n}\",\n  \"Introduction/permutations.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\nint main(){\\n    ios::sync_with_stdio(0);\\n    cin.tie(0);\\n    int n;\\n    cin >> n;\\n    if(n == 1){\\n        cout << \\\"1\\\"<<endl;\\n        return 0;\\n    }\\n    if(n <= 3){\\n        cout <<\\\"NO SOLUTION\\\"<<endl;\\n        return 0;\\n    }\\n    // print all odd then all even numbers\\n    // as odd numbers and even number have the difference of 2 and never 1 so we just check if the difference between the last odd and the first even has a difference more than 1 or not\\n    // for 4 start from n and subtract till 0\\n\\n    if(n == 4){\\n        cout << \\\"2 4 1 3\\\"<< endl;\\n        return 0;\\n    }\\n    for(int i=1;i <= n;i+=2){\\n        cout << i << \\\" \\\";\\n    }\\n    for(int i = 2;i <= n;i+=2){\\n        cout << i << \\\" \\\";\\n    }\\n    cout << endl;\\n    return 0;\\n}\",\n  \"Introduction/repetitions.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\nint main()\\n{\\n\\n    string s;\\n    cin >> s;\\n    int ans = 1;\\n    for(int i=1; i <s.length();i++){\\n        int node = 1;\\n        while(i < s.length() && s[i] == s[i-1]){\\n            node++;\\n            i++;\\n        }\\n        ans = max(ans, node);\\n    }\\n    cout << ans<< endl;\\n   return 0;\\n}\",\n  \"Introduction/string_matching.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\nint main(){\\n    ios::sync_with_stdio(0);\\n    cin.tie(0);\\n\\n    string s;\\n    string p;\\n\\n    cin >> s;\\n    cin >> p;\\n    int n=0, m=0;\\n    int ans = 0;\\n    for(int i=0;i < n;i++){\\n        int j = i;\\n        while(j < m && i+j < n && s[i+j] == p[j]){\\n            j++;\\n        }\\n        if(j == m){\\n            i = j;\\n            ans++;\\n        }\\n    }\\n    cout << ans << endl;\\n    return 0;\\n}\",\n  \"Introduction/tower_of_hanoi.cpp\": \"#include <iostream>\\n#include <fstream>\\n#include <sstream>\\n#include <iomanip>\\n#include <string>  \\n#include <vector>\\n#include <list>        \\n#include <set> \\n#include <map>    \\n#include <queue> \\n#include <stack>\\n#include <algorithm>  \\n#include <cmath> \\n#include <ctime> \\n#include <cstdlib>\\n#include <cstring> \\n#include <cctype> \\n#include <cassert>\\n#include <exception>   \\n#include <functional>\\n#include <iterator>\\n#include <limits>  \\n#include <locale>   \\n#include <numeric>  \\n#include <random> \\n#include <stdexcept> \\n#include <typeinfo> \\n#include <utility>\\nusing namespace std;\\n\\nvoid toh(int n, char s, char a, char d){\\n    if(n == 1){\\n        cout << s << \\\" \\\" << d << endl;\\n    }\\n    else{\\n        toh(n-1, s, d, a);\\n        cout << s << \\\" \\\" << d << endl;\\n        toh(n-1, a, s, d);\\n    }\\n}\\n\\nint main()\\n{\\n    cin.tie(0);\\n    int n;\\n    cin >> n;\\n    cout << pow(2, n) - 1 <<endl;\\n    toh(n, '1', '2', '3');\\n    return 0;\\n}\",\n  \"Introduction/trailing_zeros.cpp\": \"#include <iostream>\\n#include <fstream>\\n#include <sstream>\\n#include <iomanip>\\n#include <string>  \\n#include <vector>\\n#include <list>        \\n#include <set> \\n#include <map>    \\n#include <queue> \\n#include <stack>\\n#include <algorithm>  \\n#include <cmath> \\n#include <ctime> \\n#include <cstdlib>\\n#include <cstring> \\n#include <cctype> \\n#include <cassert>\\n#include <exception>   \\n#include <functional>\\n#include <iterator>\\n#include <limits>  \\n#include <locale>   \\n#include <numeric>  \\n#include <random> \\n#include <stdexcept> \\n#include <typeinfo> \\n#include <utility>\\nusing namespace std;\\nint main()\\n{\\n    int n;\\n    cin >> n;\\n    int ans = 0;\\n    while(n){\\n        n /= 5;\\n        ans += n;\\n    }\\n    cout << ans << endl;\\n    return 0;\\n}\",\n  \"Introduction/two_knights.cpp\": \"#include <iostream>\\n#include <fstream>\\n#include <sstream>\\n#include <iomanip>\\n#include <string>  \\n#include <vector>\\n#include <list>        \\n#include <set> \\n#include <map>    \\n#include <queue> \\n#include <stack>\\n#include <algorithm>  \\n#include <cmath> \\n#include <ctime> \\n#include <cstdlib>\\n#include <cstring> \\n#include <cctype> \\n#include <cassert>\\n#include <exception>   \\n#include <functional>\\n#include <iterator>\\n#include <limits>  \\n#include <locale>   \\n#include <numeric>  \\n#include <random> \\n#include <stdexcept> \\n#include <typeinfo> \\n#include <utility>\\nusing namespace std;\\nint main()\\n{\\n    int n;\\n\\n    cin >> n;\\n\\n    for(int i = 1; i <= n;i++){\\n        cout << ((long long)i*i*((long long)i*i - 1))/2 - (long long)4*(i-1)*(i-2) << endl;\\n    }\\n    return 0;\\n}\",\n  \"Introduction/word_combinations.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\n\\nint solve(string &str, vector<string>& arr, int index, string ans, unordered_map<string, int>&dp){\\n    int n = arr.size();\\n    if(ans.length() > str.length()) return 0;\\n    if(ans == str) return 1;\\n    if(index == n) return 0;\\n    if(dp.find(ans) != dp.end())return dp[ans];\\n    int node = 0;\\n    for(int i= index;i < n;i++){\\n        node += solve(str, arr, i, ans + arr[i], dp);\\n    }\\n    return dp[ans] = node;\\n}\\n\\nint main(){\\n    ios::sync_with_stdio(0);\\n    cin.tie(0);\\n    string str;\\n    cin >> str;\\n\\n    int k;\\n    cin >> k;\\n    vector<string> arr;\\n    for(int i=0;i < k; i++){\\n        string val;\\n        cin >> val;\\n        arr.push_back(val);\\n    }\\n\\n    // check in how many ways the string can be made using a dp table with unordered_map, string as key\\n\\n    unordered_map<string, int> dp;\\n\\n\\n    cout << solve(str, arr, 0, \\\"\\\", dp) <<endl;\\n    return 0;\\n}\",\n  \"bit_inversions.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\nint main(){\\n    ios::sync_with_stdio(0);\\n    cin.tie(0);\\n\\n    string s;\\n    int m = 0;\\n    cin >> s;\\n    int n = s.length();\\n    cin >> m;\\n    vector<int> changes(m, 0);\\n\\n    for(int i = 0;i < m;i++) cin >> changes[i];\\n\\n    \\n\\n    return 0;\\n}\",\n  \"goodbye2024/1.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\nint main(){\\n    ios::sync_with_stdio(0);\\n    cin.tie(0);\\n\\n    int t;\\n    cin >> t;\\n    while(t--){\\n        int n;\\n        cin >> n;\\n        vector<int> arr(n);\\n        for(int i=0;i < n;i++) cin >> arr[i];\\n\\n        // sort(arr.begin(), arr.end());\\n\\n        // if(arr[0] == arr[n-1]){\\n        //     cout << \\\"YES\\\\n\\\";\\n        //     continue;\\n        // }\\n\\n        // if(arr[0]*2<= arr[1]){\\n        //     cout << \\\"NO\\\\n\\\";\\n        // }\\n        // else cout << \\\"YES\\\\n\\\";\\n        bool possible = false;\\n        for(int i=0;i < n-1;i++){\\n            if((arr[i] + arr[i+1] > arr[i]) && (arr[i]*2 > arr[i+1])&&(arr[i+1] *2 > arr[i]) && (arr[i]+arr[i+1] > arr[i+1])){\\n                possible = true;\\n                break;\\n            }\\n        }\\n        if(possible) cout << \\\"YES\\\\n\\\";\\n        else cout <<\\\"NO\\\\n\\\";\\n    }\\n    return 0;\\n}\",\n  \"misc/bcount/bcount.in\": \"6 3\\n2\\n1\\n1\\n3\\n2\\n1\\n1 6\\n3 3\\n2 4\",\n  \"misc/bcount/bcount.out\": \"3 2 1\\n1 0 0\\n2 0 1\\n\",\n  \"misc/books.cpp\": \"#include <iostream>\\n#include <fstream>\\n#include <sstream>\\n#include <iomanip>\\n#include <string>  \\n#include <vector>\\n#include <list>        \\n#include <set> \\n#include <map>    \\n#include <queue> \\n#include <stack>\\n#include <algorithm>  \\n#include <cmath> \\n#include <ctime> \\n#include <cstdlib>\\n#include <cstring> \\n#include <cctype> \\n#include <cassert>\\n#include <exception>   \\n#include <functional>\\n#include <iterator>\\n#include <limits>  \\n#include <locale>   \\n#include <numeric>  \\n#include <random> \\n#include <stdexcept> \\n#include <typeinfo> \\n#include <utility>\\nusing namespace std;\\nint main()\\n{\\n    int n, t;\\n    cin >> n >> t;\\n    vector<int> arr(n+1);\\n    for(int i=0;i < n;i++) cin >> arr[i+1];\\n\\n    int l = 1;\\n    int r = 1;\\n    int ans = 0;\\n    while(r <= n){\\n        if(t >= arr[r]){\\n            t-=arr[r];\\n            r++;\\n        }\\n        else{\\n            t+= arr[l];\\n            l++;\\n        }\\n        ans = max(ans, r-l);\\n    }\\n    cout << ans << endl;\\n    return 0;\\n}\",\n  \"misc/closing/closing.in\": \"4 3\\n1 2\\n2 3\\n3 4\\n3\\n4\\n1\\n2\",\n  \"misc/closing/closing.out\": \"YES\\nNO\\nYES\\nYES\\n\",\n  \"misc/div7/div7.in\": \"7\\n3\\n5\\n1\\n6\\n2\\n14\\n10\",\n  \"misc/div7/div7.out\": \"5\",\n  \"misc/edu173/1.cpp\": \"#include <iostream>\\n#include <fstream>\\n#include <sstream>\\n#include <iomanip>\\n#include <string>  \\n#include <vector>\\n#include <list>        \\n#include <set> \\n#include <map>    \\n#include <queue> \\n#include <stack>\\n#include <algorithm>  \\n#include <cmath> \\n#include <ctime> \\n#include <cstdlib>\\n#include <cstring> \\n#include <cctype> \\n#include <cassert>\\n#include <exception>   \\n#include <functional>\\n#include <iterator>\\n#include <limits>  \\n#include <locale>   \\n#include <numeric>  \\n#include <random> \\n#include <stdexcept> \\n#include <typeinfo> \\n#include <utility>\\nusing namespace std;\\n\\nlong long solve(long long n){\\n    // if(n <= 4) return 1;\\n    // return 2*solve(n/4);\\n    long long curr = 1;\\n    while(n>=4){\\n        curr = 2*curr;\\n        n = n/4;\\n    }\\n    return curr;\\n}\\n\\nint main()\\n{\\n    int t;\\n    cin >> t;\\n    while(t--){\\n        long long n;\\n        cin >> n;\\n        cout << solve(n)<< '\\\\n';\\n    }\\n    return 0;\\n}\",\n  \"misc/fenceplan/fenceplan.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\nint main(){\\n    ios::sync_with_stdio(0);\\n    cin.tie(0);\\n\\n    \\n\\n    return 0;\\n}\",\n  \"misc/fenceplan/fenceplan.in\": \"7 5\\n0 5\\n10 5\\n5 0\\n5 10\\n6 7\\n8 6\\n8 4\\n1 2\\n2 3\\n3 4\\n5 6\\n7 6\",\n  \"misc/lazycow/lazy.in\": \"5 2\\n50 5 25 6 17\\n14 3 2 7 21\\n99 10 1 2 80\\n8 7 5 23 11\\n10 0 78 1 9\",\n  \"misc/lazycow/lazy.out\": \"342\",\n  \"misc/little_girl_max_sum.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\nint main(){\\n    int n, q;\\n    long long ans = 0;\\n    cin >> n >> q;\\n    vector<int> arr(n+1, 0);\\n    for(int i = 1;i <= n;i++) cin >> arr[i];\\n    sort(arr.begin()+1, arr.end());\\n    vector<long long> queries(n+2, 0);\\n    for(int i = 1;i <= q;i++){\\n        int x, y;\\n        cin >> x >> y;\\n        queries[x]++;\\n        queries[y+1]--;\\n    }\\n\\n    for(int i = 1;i <= n;i++) queries[i] += (long long)queries[i-1];\\n    sort(queries.begin() + 1, queries.end() - 1);\\n    // the queries array tells us how many times the index is repeated in all the queries there we sort it and multiply max value with max value which works\\n    // else do it with BIT(Binary Indexed Trees)\\n    for(int i = 1;i <= n;i++) ans += (long long)arr[i]*queries[i];\\n    cout << ans << endl;\\n    return 0;\\n}\",\n  \"misc/moocast/moocast.in\": \"4\\n1 3 5\\n5 4 3\\n7 2 1\\n6 1 1\",\n  \"misc/moocast/moocast.out\": \"3\",\n  \"misc/notlast/notlast.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\nint main(){\\n    ios::sync_with_stdio(0);\\n    cin.tie(0);\\n    ifstream fin(\\\"notlast.in\\\");\\n    ofstream fin(\\\"notlast.out\\\");\\n    int n;\\n    fin >> n;\\n\\n    for(){\\n        \\n    }\\n    return 0;\\n}\",\n  \"misc/notlast/notlast.in\": \"\",\n  \"misc/paintbarn/paintbarn.in\": \"3 2\\n1 1 5 5\\n4 4 7 6\\n3 3 8 7\",\n  \"misc/paintbarn/paintbarn.out\": \"0 0 0 0 0 0 0 0 0 0 \\n0 1 0 0 0 -1 0 0 0 0 \\n0 0 0 0 0 0 0 0 0 0 \\n0 0 0 1 0 0 0 -1 0 0 \\n0 0 0 0 1 0 -1 0 0 0 \\n0 -1 0 0 0 1 0 0 0 0 \\n0 0 0 0 0 0 0 0 0 0 \\n0 0 0 0 -1 0 1 0 0 0 \\n0 0 0 -1 0 0 0 1 0 0 \\n0 0 0 0 0 0 0 0 0 0 \\n\\n\\n0 0 0 0 0 0 0 0 0 0 \\n0 0 0 0 0 0 0 0 0 0 \\n0 0 1 1 1 1 0 0 0 0 \\n0 0 1 1 1 1 0 0 0 0 \\n0 0 1 1 2 2 1 1 0 0 \\n0 0 1 1 2 3 2 1 0 0 \\n0 0 0 0 1 2 2 1 0 0 \\n0 0 0 0 1 2 2 1 0 0 \\n0 0 0 0 1 1 1 1 0 0 \\n0 0 0 0 0 0 0 0 0 0 \\n\\n0 0 0 0 0 0 0 0 0 0 \\n0 1 1 1 1 0 0 0 0 0 \\n0 1 1 1 1 0 0 0 0 0 \\n0 1 1 2 2 1 1 0 0 0 \\n0 1 1 2 3 2 1 0 0 0 \\n0 0 0 1 2 2 1 0 0 0 \\n0 0 0 1 2 2 1 0 0 0 \\n0 0 0 1 1 1 1 0 0 0 \\n0 0 0 0 0 0 0 0 0 0 \\n0 0 0 0 0 0 0 0 0 0 \\n8\",\n  \"misc/pairup/pairup.in\": \"3\\n1 8\\n2 5\\n1 2\",\n  \"misc/pairup/pairup.out\": \"10\\n\",\n  \"misc/permutator.cpp\": \"#include <iostream>\\n#include <fstream>\\n#include <sstream>\\n#include <iomanip>\\n#include <string>  \\n#include <vector>\\n#include <list>        \\n#include <set> \\n#include <map>    \\n#include <queue> \\n#include <stack>\\n#include <algorithm>  \\n#include <cmath> \\n#include <ctime> \\n#include <cstdlib>\\n#include <cstring> \\n#include <cctype> \\n#include <cassert>\\n#include <exception>   \\n#include <functional>\\n#include <iterator>\\n#include <limits>  \\n#include <locale>   \\n#include <numeric>  \\n#include <random> \\n#include <stdexcept> \\n#include <typeinfo> \\n#include <utility>\\nusing namespace std;\\nint main()\\n{\\n    \\n    return 0;   \\n}\",\n  \"misc/test.out\": \"0 0 0 0 0 0 \\n0 1 1 1 2 0 \\n0 1 1 1 2 0 \\n0 2 1 1 3 0 \\n0 0 0 0 0 0 \\n3201\",\n  \"misc/usaco/range_sum.cpp\": \"#include <iostream>\\n#include <fstream>\\n#include <sstream>\\n#include <iomanip>\\n#include <string>  \\n#include <vector>\\n#include <list>        \\n#include <set> \\n#include <map>    \\n#include <queue> \\n#include <stack>\\n#include <algorithm>  \\n#include <cmath> \\n#include <ctime> \\n#include <cstdlib>\\n#include <cstring> \\n#include <cctype> \\n#include <cassert>\\n#include <exception>   \\n#include <functional>\\n#include <iterator>\\n#include <limits>  \\n#include <locale>   \\n#include <numeric>  \\n#include <random> \\n#include <stdexcept> \\n#include <typeinfo> \\n#include <utility>\\nusing namespace std;\\nint main()\\n{\\n    int n, q;\\n    cin >> n >> q;\\n    vector<long> arr(n);\\n    for(int i=0;i < n;i++){\\n        cin >> arr[i];\\n    }\\n\\n    for(int i=1;i < n;i++){\\n        arr[i] = arr[i]+arr[i-1];\\n    }\\n\\n    vector<long> ans(q);\\n\\n    for(int i=0;i < q;i++){\\n        int l, r;\\n        cin >> l >> r;\\n        ans[i] = (l > 0)?(arr[r-1] - arr[l-1]): arr[r-1];\\n    }\\n\\n    for(long i: ans) cout << i << endl;\\n    return 0;\\n}\",\n  \"misc/where_am_i/where_am_i.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\nint main(){\\n    ios::sync_with_stdio(0);\\n    cin.tie(0);\\n    ifstream fin(\\\"whereami.in\\\");\\n    ofstream fout(\\\"whereami.out\\\");\\n    int n;\\n    string s;\\n    fin >> n >> s;\\n\\n    for(int i=0;i < n;i++){\\n        unordered_map<string, int> m;\\n        // all the substring lengths\\n        bool flag = true;\\n        for(int j = 0;j < n - i;j++){\\n            string s1 = s.substr(j, i+1);\\n            m[s1]++;\\n            if(m[s1] > 1){\\n                flag = false;\\n                break;\\n            }\\n        }\\n        if(flag){\\n            fout << i+1;\\n            return 0;\\n        }\\n    }\\n    return 0;\\n}\",\n  \"misc/where_am_i/whereami.in\": \"7\\nABCDABC\",\n  \"misc/where_am_i/whereami.out\": \"4\",\n  \"problem_sets/.vscode/settings.json\": \"{\\n    \\\"files.associations\\\": {\\n        \\\"iostream\\\": \\\"cpp\\\"\\n    },\\n    \\\"C_Cpp.errorSquiggles\\\": \\\"enabled\\\"\\n}\",\n  \"problem_sets/MissingNumber.cpp\": \"#include<iostream>\\nusing namespace std;\\n\\nvoid swap(int *x, int *y){\\n    int temp = *x;\\n    *x=*y;\\n    *y=temp;\\n}\\n\\nint main(){\\n    int n,j,i;\\n    cin>>n;\\n    int arr[n];\\n    for( i=0;i<n-1;i++) cin>>arr[i];\\n    for(i=0;i<n-1;i++){\\n        for(j=i;j<n-1;j++) if(arr[j]<arr[i]) swap(&arr[j],&arr[i]);\\n    }\\n    for (i=0;i<n-1;i++) if (arr[i]!=i+1){\\n        cout<<i+1;\\n        break;\\n    }\\n    return 0;\\n}\",\n  \"problem_sets/WeirdAlgorithm.cpp\": \"#include<iostream>\\nusing namespace std;\\nlong long int recursion(long long int n){\\n    if (n==1) return 1;\\n    else if(n>0 && n%2==0){\\n        cout<<n/2<<\\\" \\\";\\n        return recursion(n/2);\\n    }\\n    else {\\n        cout<<3*n+1<<\\\" \\\";\\n        return recursion(3*n+1);\\n    }\\n}\\n\\nint main(){\\n    long long int n;\\n    cin>>n;\\n    cout << n<< \\\" \\\";\\n    recursion(n);\\n    return 0;\\n}\\n\",\n  \"problem_sets/ansh.c\": \"# include<stdio.h>\\n\\ttypedef struct address{\\n\\t\\tint houseno;\\n\\t\\tint block;\\n\\t\\tchar city[50];\\n\\t\\tchar state[50];\\n\\t}add;\\n\\tint main(){\\n\\t\\tadd op[5];\\n\\t\\tfor(int i=0;i<5;i++){\\n\\t\\t\\tprintf(\\\"enter the house number \\\");\\n\\t\\t\\tscanf(\\\"%d\\\",&op[i].houseno);\\n\\t\\t\\tprintf(\\\"enter the block\\\");\\n\\t\\t\\tscanf(\\\"%d\\\",&op[i].block);\\n\\t\\t\\tprintf(\\\"enter the city \\\");\\n\\t\\t\\tscanf(\\\" %[^\\\\n]s\\\",op[i].city);\\n\\t\\t\\tprintf(\\\"enter the state\\\");\\n\\t\\t\\tscanf(\\\" %[^\\\\n]s\\\",op[i].state);\\n\\t\\t}\\n        return 0;\\n\\n\\t}\",\n  \"problem_sets/compare_strings.c\": \"#include<stdio.h>\\n#include<string.h>\\n\\nint min(int a,int b){\\n    if(a>b) return b;\\n    else return a;\\n}\\n\\n\\nint max(int a,int b){\\n    if(a>b) return a;\\n    else return b;\\n}\\n\\nint main(){\\n    char a[100],b[100];\\n    gets(a);\\n    gets(b);\\n    char c[100], d[100], e[100];\\n    int i,l1,l2,j;\\n    for(i=0;a[i]!='\\\\0';i++);\\n    l1=i;\\n    for(i=0;b[i]!='\\\\0';i++);\\n    l2=i;\\n    int maxi= max(l1,l2);\\n    for(i=0;i<maxi;i++){\\n        if(a[i]==b[i]) c[i]= a[i];\\n    }\\n    puts(c);\\n    i=0;\\n    while(i<maxi){\\n        if(c[i]!=b[i]) printf(\\\"%c\\\",b[i]);\\n        i++;\\n    }\\n    printf(\\\"\\\\n\\\");\\n    i=0;\\n    while(i<maxi){\\n        if(c[i]!=a[i]) printf(\\\"%c\\\",a[i]);\\n        i++;\\n    }\\n    return 0;    \\n}\\n\",\n  \"problem_sets/cricket.c\": \"#include <stdio.h>\\n#include <string.h>\\n\\ntypedef struct cricket {\\n  char player_name[20];\\n  char team_name[20];\\n  float batting_avg;\\n} cricketer;\\n\\n\\n\\nint main(void) {\\n  int i = 0, j = 0, n = 10;\\n    cricketer player[50];\\n  for (i = 0; i < n; i++) {\\n    printf(\\\"\\\\n Enter Player Name : \\\");\\n    scanf(\\\"%s\\\", player[i].player_name);\\n    printf(\\\"\\\\n Enter Team Name : \\\");\\n    scanf(\\\"%s\\\", player[i].team_name);\\n    printf(\\\"\\\\n Enter Batting Average : \\\");\\n    scanf(\\\"%f\\\", & player[i].batting_avg);\\n  }\\n\\n  return 0;\\n}\\n\\n\",\n  \"problem_sets/doc.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\nint main()\\n{\\n    int n;\\n    cin >> n;\\n    unordered_set<int> s;\\n    for(int i=0;i < n-1;i++){\\n        int val;\\n        cin >> val;\\n        s.insert(val);\\n    }\\n\\n    for(int i=1;i <= n;i++){\\n        if(s.find(i) == s.end()){\\n            cout << i <<endl;\\n            return 1;\\n        }\\n    }\\n    return 0;\\n}\",\n  \"problem_sets/findmedian.c\": \"double findMedianSortedArrays(int* nums1, int nums1Size, int* nums2, int nums2Size) {\\n    long long n_check= nums2[0];\\n    long long s=0,l,mid,index;\\n    l=nums1Size;\\n    mid =(s+l)/2;\\n    while(l>=s){\\n        if(nums1[mid]>n_check) l=mid-1;\\n        else if (nums1[mid]<n_check) s=mid+1;\\n        else{\\n            index= mid;\\n            break;\\n        }\\n        mid = (s+l)/2;\\n    }\\n    \\n    for(long long i=nums1Size;i>index;i++){\\n        \\n    }\\n\\n    if ((nums1Size+nums2Size)%2==0) return(nums1[(nums1Size+nums2Size)/2-1]+nums1[(nums1Size+nums2Size)/2]);\\n    else return(nums1[nums1Size+nums2Size]);\\n}\",\n  \"problem_sets/increasingarray.cpp\": \"#include<iostream>\\nusing namespace std;\\nint main(){\\n    long long count=0;\\n    long long n;\\n    cin>>n;\\n    int arr[n];\\n    for(long long i=0;i<n;i++) cin>>arr[i];\\n    for(long long i=1;i<n;i++){\\n        while(arr[i]<=arr[i-1]){\\n            arr[i]++;\\n            count++;\\n        }\\n    }\\n    cout<<count<<endl;\\n    return 0;\\n}\",\n  \"problem_sets/repitition.cpp\": \"#include<iostream>\\n#include<string>\\nusing namespace  std;\\nint main(){\\n    long long streak=0,temp=0;\\n    string str;\\n    cin>>str;// otherwise getline() function is to be used\\n\\n    for(long long i=0;i<str.size()-1;i++){\\n        for(long long j=i;j<str.size();j++){\\n            if(str[i]==str[j]){\\n                    temp++;\\n                    streak=max(temp,streak);\\n                }\\n            else{\\n                    temp=0;\\n                    break;\\n                }\\n        }\\n    }\\n    cout<<streak<<endl;\\n    return 0;\\n}\\n\\nlong long max(long long a,long long b){\\n    long long maxi=INT16_MIN;\\n    if(a>b) maxi=a;\\n    else maxi= b;\\n    return maxi;\\n}\",\n  \"range_queries/forest_queries.cpp\": \"// #include<bits/stdc++.h>\\n#include<vector>\\n#include<algorithm>\\n#include<iostream>\\nusing namespace std;\\nint main(){\\n    ios::sync_with_stdio(0);\\n    cin.tie(0);\\n    int n, q;\\n    cin >> n >> q;\\n    vector<vector<int>> arr(n+1, vector<int>(n+1, 0));\\n\\n    for(int i = 1;i <= n;i++){\\n        for(int j = 1;j <= n;j++){\\n            char c;\\n            cin >> c;\\n            if(c == '*'){\\n                arr[i][j] = 1;\\n            }\\n        }\\n    }\\n\\n    for(int i = 1;i <= n;i++){\\n        for(int j = 1;j <= n;j++){\\n            arr[i][j] = arr[i][j] + arr[i-1][j] + arr[i][j-1] - arr[i-1][j-1];\\n        }\\n    }\\n\\n    for(int i = 0;i < q;i++){\\n        int x1, y1, x2, y2;\\n        cin >> x1 >> y1 >> x2 >> y2;\\n        cout << arr[x2][y2] - arr[x1-1][y2] - arr[x2][y1-1] + arr[x1-1][y1-1] << endl;\\n    }\\n\\n    return 0;\\n}\",\n  \"sorting_searching/concert_tickets.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\nint main(){\\n    ios::sync_with_stdio(0);\\n    cin.tie(0);\\n    int n, m;\\n    cin >> n >> m;\\n    vector<long long> temp(n, 0);\\n    for(int i = 0;i < n;i++){\\n        cin >> temp[i];\\n    }\\n    map<long long, long long> s;\\n\\n    for(int i = 0;i < n;i++){\\n        s[temp[i]]++;\\n    }\\n\\n    vector<long long> arr(m, 0);\\n    for(int i = 0;i < m;i++){\\n        cin >> arr[i];\\n    }\\n\\n    for(int i = 0;i < m;i++){\\n        int val = arr[i];\\n        if(s.empty()){cout <<\\\"-1\\\\n\\\"; continue;}\\n        auto maxx = s.upper_bound(val);\\n\\n        if(maxx == s.begin()){\\n            cout << \\\"-1\\\\n\\\";\\n        }\\n        else{\\n            maxx--;\\n            cout << (maxx->first) << '\\\\n';\\n            s[maxx->first]--;\\n            if(!s[maxx->first]) s.erase(maxx);\\n        }\\n    }\\n    return 0;\\n}\",\n  \"sorting_searching/room_allocation.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\nusing pii = pair<int, int>;\\nint main(){\\n    ios::sync_with_stdio(0);\\n    cin.tie(0);\\n    int n;\\n    cin >> n;\\n    vector<int> rooms(n, 0);\\n    vector<pii> times(n);\\n    for(int i = 0;i < n;i++){\\n        cin >> times[i].first;\\n        cin >> times[i].second;\\n    }\\n    sort(times.begin(), times.end());\\n\\n    multiset<int> m;\\n\\n    for(int i = 0;i < n;i++){\\n        \\n    }\\n    return 0;\\n}\",\n  \"sorting_searching/subarray_sum1.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\nint main()\\n{\\n    long long ans = 0;\\n    map<long long, long long> m;\\n    m[0] = 1;\\n    int n, x;\\n    cin >> n >> x;\\n    vector<int> arr(n);\\n    for(int i = 0;i < n;i++) cin >> arr[i];\\n\\n    long long suma = 0;\\n    for(int i = 0;i < n;i++){\\n        suma += arr[i];\\n        ans += m[suma - x];\\n        m[suma]++;\\n    }\\n    cout << ans << endl;\\n    return 0;\\n}\",\n  \"sorting_searching/traffic_lights.cpp\": \"#include<bits/stdc++.h>\\nusing namespace std;\\nint main(){\\n    ios::sync_with_stdio(0);\\n    cin.tie(0);\\n\\n    int x, n;\\n    cin >> x >> n;\\n\\n    multiset<int> m;\\n    m.insert(x);\\n\\n    set<int> s;\\n    s.insert(0);\\n    s.insert(x);\\n\\n    for(int i = 0;i < n;i++){\\n        int pos;\\n        cin >> pos;\\n\\n        auto left = s.upper_bound(pos);\\n        auto right = left;\\n        left--;\\n\\n        m.erase(m.find((*right) - (*left)));\\n        m.insert((*right) - pos);\\n        m.insert(pos - (*left));\\n        s.insert(pos);\\n        cout << *(--m.end()) << ' ';\\n    }\\n\\n    return 0;\\n}\"\n}\n", "misc_bin/demo.py": "", "misc_bin/maybe_needed_later/try_sum.py": "# from llama_cpp import Llama\n# import json\n\n# # Load JSON data\n# with open('files_data.json', 'r') as file:\n#     data = json.load(file)\n\n# # Initialize the LLM (adjust path to your model)\n# llm = Llama(model_path=\"/Users/devansh/Folder_1/mymodels/ollama/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff.gguf\", n_ctx=4096) # Example: \"/Users/username/models/llama-2-7b-chat.Q4_K_M.gguf\"\n\n# def summarize_file(filename, content):\n#     with open(\"check.txt\", 'a') as f:\n#         f.write(f\"{filename}: doing\\n\")\n#     prompt = f\"Summarize the following file, in 30 words: {filename}:\\n\\n{content}\\n\\nSummary:\"\n#     output = llm(prompt, max_tokens=100, stop=[\"Q:\", \"\\n\"], echo=False)\n#     return output['choices'][0]['text'].strip()\n\n# file_summaries = {}\n# for filename, content in data.items():\n#     file_summaries[filename] = summarize_file(filename, content)\n\n# # Print the summaries\n\n# with open('final_summaries.json', 'w') as f:\n#     json.dump(file_summaries, f)\n\n# for filename, summary in file_summaries.items():\n#     print(f\"File: {filename}\\nSummary: {summary}\\n\")\n\n\n# import ollama\n\n# # Initialize the client\n# client = ollama.Client()\n\n# # Generate a response\n# response = client.generate(model='llama3.2:latest', prompt='What is the capital of France?')\n\n# print(response['response'])\n\n\n# import subprocess\n# import json\n\n# def run_ollama(prompt, model='llama3.2:latest'):\n#     command = ['ollama', 'run', model, prompt]\n#     result = subprocess.run(command, capture_output=True, text=True)\n#     return result.stdout.strip()\n\n# # Example usage\n# response = run_ollama('What is the capital of France?')\n# print(response)\n", "misc_bin/neww.json": "{\n  \"data\": {\n    \"repository\": {\n      \"object\": {\n        \"entries\": [\n          { \"name\": \".github\", \"type\": \"tree\", \"mode\": 16384, \"object\": {} },\n          {\n            \"name\": \".gitignore\",\n            \"type\": \"blob\",\n            \"mode\": 33188,\n            \"object\": {\n              \"text\": \".env\\n.vscode\\n.DS_STORE\\npnpm-lock.yaml\\nyarn.lock\\npackage-lock.json\\nnode_modules\\n.turbo\\n\",\n              \"byteSize\": 86\n            }\n          },\n          { \"name\": \".vscode\", \"type\": \"tree\", \"mode\": 16384, \"object\": {} },\n          {\n            \"name\": \"CODE_OF_CONDUCT.md\",\n            \"type\": \"blob\",\n            \"mode\": 33188,\n            \"object\": {\n              \"text\": \"# Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\nWe are committed to creating a welcoming, harassment-free experience for everyone. Contributors pledge to act respectfully and professionally.\\n\\n## Our Standards\\nExamples of behavior that contributes to a positive environment include:\\n- Using welcoming and inclusive language.\\n- Being respectful of differing viewpoints and experiences.\\n- Accepting constructive feedback.\\n\\nExamples of unacceptable behavior include:\\n- The use of sexualized language or imagery.\\n- Trolling, insulting/derogatory comments, and personal attacks.\\n- Public or private harassment.\\n\\n## Our Responsibilities\\nProject maintainers are responsible for enforcing the code of conduct and may take appropriate action if violations occur.\\n\\n## Enforcement\\nInstances of abusive, harassing, or otherwise unacceptable behavior can be reported by contacting the project team at **hyattherate2005 [at] gmail.com**.\\n\",\n              \"byteSize\": 929\n            }\n          },\n          {\n            \"name\": \"CONTRIBUTING.md\",\n            \"type\": \"blob\",\n            \"mode\": 33188,\n            \"object\": {\n              \"text\": \"# Contributing to NoterAI\\n\\nThank you for considering contributing to NoterAI! We\\u2019re excited to have you on board. Before you start, please read our contribution guidelines carefully.\\n\\n## Table of Contents\\n- [Code of Conduct](#code-of-conduct)\\n- [How Can You Contribute?](#how-can-you-contribute)\\n- [Getting Started](#getting-started)\\n- [Installation Guide](#installation-guide)\\n- [Coding Guidelines](#coding-guidelines)\\n- [Commit Message Guidelines](#commit-message-guidelines)\\n- [Submitting Issues](#submitting-issues)\\n- [Pull Request Guidelines](#pull-request-guidelines)\\n- [License](#license)\\n- [Contact](#contact)\\n\\n## Code of Conduct\\nPlease make sure to review and adhere to our [Code of Conduct](./CODE_OF_CONDUCT.md) to ensure a respectful and welcoming environment for all contributors.\\n\\n## How Can You Contribute?\\nWe appreciate any type of contribution:\\n- **Report Bugs**: Submit bugs via our GitHub issue tracker.\\n- **Feature Requests**: Have ideas for new features? Share them via GitHub.\\n- **Code Contributions**: Help us by fixing bugs, writing new features, or improving the documentation.\\n- **Documentation**: Help improve the documentation to make the project more accessible.\\n\\n## Getting Started\\n\\n### Prerequisites\\nEnsure you have the following installed:\\n- **Node.js** (>= version X.X.X)\\n- **npm** (>= version X.X.X)\\n- **Flutter** (>= version X.X.X)\\n- **pnpm** (>= version X.X.X)\\n- **Docker** (for setting up Redis and the database)\\n\\nFor platform-specific requirements:\\n- **iOS/macOS**: Xcode for Swift and CocoaPods setup.\\n- **Android**: Android Studio and necessary SDKs.\\n\\n### Tech Stack\\n- **Backend**: NestJS, Prisma, Redis\\n- **Mobile (Android, Windows, Linux)**: Flutter\\n- **iOS, macOS**: Swift\\n- **Web**: Next.js\\n- **AI**: NestJS (TypeScript), Python\\n\\n### Packages Structure\\n- **<project-root>/dev-docs**: Documentation related to the project.\\n- **<project-root>/apps/flutter**: Flutter code for mobile and desktop.\\n- **<project-root>/apps/backend**: Backend code organized in a microservices architecture.\\n- **<project-root>/apps/swift**: Swift code for iOS/macOS.\\n- **<project-root>/apps/web**: Web code (Next.js).\\n\\nFor detailed installation instructions, refer to the [Installation Guide](./INSTALLATION_GUIDE.md).\\n\\n## Installation Guide\\nRefer to our [Installation Guide](./INSTALLATION_GUIDE.md) for detailed setup instructions.\\n\\n## Coding Guidelines\\nWe follow TDD, clean code principles, and modular architecture. Make sure your code is:\\n- Well-documented.\\n- Includes tests.\\n- Passes linting and formatting checks.\\n\\nRun the following before committing:\\n```bash\\npnpm format\\npnpm lint\\n```\\n\\n### Pre-Commit Hooks\\nWe use pre-commit hooks to ensure code quality:\\n```bash\\n\\\"pre-commit\\\": \\\"pnpm format && pnpm lint && pnpm build\\\"\\n```\\n\\n## Commit Message Guidelines\\nFollow the [Conventional Commits](https://www.conventionalcommits.org/) format:\\n- **feat**: New feature.\\n- **fix**: Bug fix.\\n- **docs**: Documentation update.\\n- **style**: Code formatting changes.\\n- **refactor**: Code refactoring without changing functionality.\\n- **test**: Adding or modifying tests.\\n\\n## Submitting Issues\\nTo submit an issue, follow these steps:\\n1. **Search for duplicates**: Check if an issue already exists.\\n2. **Describe the issue**: Provide a detailed description, steps to reproduce, and any relevant screenshots.\\n3. **Suggest potential fixes**: If applicable, propose a solution.\\n\\n## Pull Request Guidelines\\n1. Fork the repository and create a new branch.\\n2. Write clean, modular, and test-driven code.\\n3. Update the documentation if necessary.\\n4. Run tests and ensure all checks pass.\\n5. Open a pull request with a clear description of the changes.\\n\\n## License\\nThis project is licensed under a **Custom License** that restricts commercial use. See the [LICENSE](./LICENSE) file for more details.\\n\\n## Contact\\nIf you have any questions, feel free to contact us at **hyattherate2005 [at] gmail.com**.\\n\",\n              \"byteSize\": 3905\n            }\n          },\n          {\n            \"name\": \"INSTALLATION_GUIDE.md\",\n            \"type\": \"blob\",\n            \"mode\": 33188,\n            \"object\": {\n              \"text\": \"# Installation Guide for NoterAI\\n\\nFollow these steps to install and run NoterAI locally.\\n\\n## Prerequisites\\nBefore setting up the project, ensure you have the following installed:\\n- **Node.js** (>= X.X.X)\\n- **npm** (>= X.X.X)\\n- **Flutter** (>= X.X.X)\\n- **pnpm** (>= X.X.X)\\n- **Docker** (for setting up Redis and Prisma database)\\n\\n### Platform-specific Requirements\\n#### iOS/macOS\\n- **Xcode**: Install Xcode via the App Store and ensure that you have the latest version.\\n- **CocoaPods**: Run `sudo gem install cocoapods` if not installed.\\n\\n#### Android\\n- **Android Studio**: Install Android Studio and set up the necessary SDKs.\\n- Ensure you have **Flutter** installed.\\n\\n## Step-by-Step Guide\\n\\n### 1. Clone the Repository\\nFirst, clone the repository to your local machine:\\n```bash\\ngit clone https://github.com/himasnhu-at/noterai.git\\ncd noterai\\n```\\n\\n### 2. Install Dependencies\\nNavigate to the project root and install the necessary dependencies using `pnpm`:\\n```bash\\npnpm install\\n```\\n\\n### 3. Set Up the Environment\\nCreate a `.env` file in the `/apps/backend` and `/apps/web` directories based on the provided `.env.example` files:\\n```bash\\ncp apps/backend/.env.example apps/backend/.env\\ncp apps/web/.env.example apps/web/.env\\n```\\nMake sure to configure your Redis and Prisma database URLs.\\n\\n### 4. Start the Backend and Frontend\\nStart both the backend and frontend using Turbo:\\n```bash\\npnpm dev\\n```\\n\\nAlternatively, use Turbo to run specific parts:\\n```bash\\n# Backend\\nturbo run dev --filter=@noterai/backend\\n\\n# Web\\nturbo run dev --filter=@noterai/web\\n```\\n\\n### 5. Running Flutter\\nNavigate to the `/apps/flutter` directory and run the project:\\n```bash\\nflutter run\\n```\\nEnsure that you have a connected device or emulator running.\\n\\n### 6. Database Setup\\nEnsure that you have Docker running for Prisma and Redis:\\n```bash\\ndocker-compose up\\n```\\n\\nThen run migrations:\\n```bash\\npnpm prisma migrate dev\\n```\\n\\n### 7. Running Tests\\nYou can run tests to ensure everything is set up correctly:\\n```bash\\npnpm test\\n```\\n\\n## Troubleshooting\\nIf you encounter issues during setup, please check our [FAQ](./docs/FAQ.md) or open an issue in the GitHub repository.\\n\",\n              \"byteSize\": 2135\n            }\n          },\n          {\n            \"name\": \"LICENSE.md\",\n            \"type\": \"blob\",\n            \"mode\": 33188,\n            \"object\": {\n              \"text\": \"# **NoterAI License Agreement**\\n\\nThis License Agreement (\\\"Agreement\\\") governs the use, distribution, and modification of the NoterAI project and its associated source code, files, and other related materials (collectively, the \\\"Software\\\"). By accessing, utilizing, or contributing to this Software, the licensee (\\\"You\\\") agrees to adhere to the terms and conditions set forth in this Agreement.\\n\\n## **1. Grant of License**  \\nSubject to the restrictions set forth in this Agreement, the author of the Software (\\\"Licensor\\\") hereby grants You a limited, non-exclusive, non-transferable, non-commercial license to use, modify, and distribute the Software solely for personal or non-commercial team purposes, under the following conditions:\\n- You are permitted to execute, compile, and use the Software for personal use or within your team for internal, non-commercial purposes only.\\n- No rights are granted to use the Software for any commercial purpose without prior written permission from the Licensor.\\n\\n## **2. Restrictions on Use**  \\nThe following restrictions apply to Your use of the Software:\\n- **Prohibition of Commercial Exploitation**: You are expressly prohibited from using the Software, in whole or in part, for any commercial purpose, including but not limited to the sale, licensing, distribution, or monetization of the Software or any derivative work thereof, without the express prior written consent of the Licensor.\\n- **Prohibition of Unauthorized Distribution**: You may not distribute, publish, sublicense, or otherwise transfer the Software, or any derivative works thereof, under Your name or the name of any other entity without the prior written consent of the Licensor. All rights not expressly granted herein are reserved by the Licensor.\\n- **Attribution Requirement**: You must retain all copyright and attribution notices in all copies or substantial portions of the Software and must provide appropriate credit to the original Licensor when redistributing any part of the Software.\\n\\n## **3. Permitted Use for Personal or Team Purposes**  \\nYou are authorized to use the Software solely for:\\n- Personal, educational, or non-commercial purposes;\\n- Deployment within a team or collaborative setting for non-commercial activities, provided that such use does not involve the sale or commercialization of the Software or its derivative works.\\n\\n## **4. Licensing of Project Components**  \\nThe Software may contain components that are governed by distinct licenses, as specified in their respective directories. Where a component of the Software is subject to a specific license:\\n- You must comply with the terms of that specific license when using, modifying, or distributing that component;\\n- In the absence of an explicitly defined license for a particular component, the terms of this **NoterAI License Agreement** shall govern its use, modification, and distribution.\\n\\nExample Licensing Structure:\\n- Backend Code: Governed under the Backend License in /<root>/LICENSE.\\n- Frontend Code: Governed under the Frontend License in /<root>/LICENSE.\\n- AI Components: Governed under the AI License in /<root>/LICENSE.\\n- Mobile Code (Flutter): Governed under the Mobile License in /<root>/LICENSE.\\n- Swift Code for iOS/macOS: Governed under the iOS/macOS License located in /<root>/LICENSE.\\n\\nIn the event of any conflict between this Agreement and a specific license, the terms of the specific license shall take precedence solely with respect to the component covered by that license.\\n\\n## **5. No Warranty and Limitation of Liability**  \\nThe Software is provided on an \\\"AS IS\\\" basis, without warranties or conditions of any kind, either express or implied, including but not limited to any warranties of merchantability, fitness for a particular purpose, or non-infringement. The Licensor shall not be liable for any damages, claims, or liabilities arising from the use or inability to use the Software, whether in contract, tort, or any other legal theory.\\n\\n## **6. Termination**  \\nYour rights under this Agreement will terminate automatically and immediately without notice from the Licensor if You fail to comply with any term or condition of this Agreement. Upon termination, You shall cease all use of the Software and destroy all copies, full or partial, of the Software in Your possession.\\n\\n## **7. Governing Law**  \\nThis Agreement shall be governed by and construed in accordance with the laws of the jurisdiction in which the Licensor resides, without regard to its conflicts of law provisions.\\n\\n## **8. Contact Information**  \\nFor any inquiries or clarifications regarding this Agreement, You may contact the Licensor at **hyattherate2005@gmail.com**.\\n\\n---\\n\\nSigned,  \\n**Himanshu**  \\nLicensor\\n\",\n              \"byteSize\": 4717\n            }\n          },\n          {\n            \"name\": \"SECURITY.md\",\n            \"type\": \"blob\",\n            \"mode\": 33188,\n            \"object\": {\n              \"text\": \"# Security Policy\\n\\n## Supported Versions\\n\\nAs of now, **NoterAI** does not have any officially released versions. Once we release our first version, this section will be updated to reflect which versions are actively supported with security updates.\\n\\n| Version | Supported          |\\n| ------- | ------------------ |\\n| N/A     | :white_check_mark: |\\n| N/A     | :x:                |\\n\\n## Reporting a Vulnerability\\n\\nIf you discover a security vulnerability in NoterAI, we encourage you to report it responsibly. Please follow these steps:\\n\\n1. **Contact**: Report the vulnerability by emailing us at **hyattherate2005 [at] gmail.com** with the subject line \\\"Security Vulnerability Report\\\".\\n2. **Provide Details**: Include as much information as possible regarding the vulnerability, including:\\n   - A detailed description of the issue.\\n   - Steps to reproduce the vulnerability (if applicable).\\n   - Any potential risks or impacts you\\u2019ve identified.\\n3. **Confirmation**: Once we receive your report, we will acknowledge receipt within 48 hours.\\n4. **Investigation**: We will conduct an internal investigation to verify and assess the vulnerability.\\n5. **Updates**: We will provide regular updates on the status of the investigation and any actions taken to address the vulnerability.\\n6. **Resolution**: After resolving the issue, we will notify you of the fix and release a security patch (if necessary). In some cases, we may publicly acknowledge your contribution, depending on your preferences.\\n\\nThank you for helping us maintain the security of NoterAI.\\n\",\n              \"byteSize\": 1556\n            }\n          },\n          {\n            \"name\": \"USAGE_GUIDE.md\",\n            \"type\": \"blob\",\n            \"mode\": 33188,\n            \"object\": {\n              \"text\": \"# NoterAI Usage Guide\\n\\nWelcome to the NoterAI usage guide! This document will walk you through the main features and functionalities of NoterAI and how to effectively use them.\\n\\n## Table of Contents\\n- [Creating and Managing Notes](#creating-and-managing-notes)\\n- [Using AI-Powered Summarization](#using-ai-powered-summarization)\\n- [Generating Quizzes](#generating-quizzes)\\n- [Asking Questions (Q&A)](#asking-questions-qna)\\n- [Viewing Interesting Facts](#viewing-interesting-facts)\\n- [Voice Input for Notes](#voice-input-for-notes)\\n- [Accessing the App](#accessing-the-app)\\n- [Settings and Customizations](#settings-and-customizations)\\n\\n## Creating and Managing Notes\\n\\n### 1. **Creating Notes**\\nNoterAI provides a Notion-like interface for creating notes. You can start by:\\n- Clicking the \\\"New Note\\\" button on the dashboard.\\n- Adding a title and typing your content in the note editor.\\n- Using rich text features like headings, bold, italics, lists, etc., to format your notes.\\n\\n### 2. **Organizing Notes**\\nYou can organize your notes in folders or tag them to make them easily searchable. To add a folder:\\n- Navigate to \\\"Folders\\\" on the left sidebar and click \\\"Create Folder.\\\"\\n- Drag and drop notes into folders or assign tags during note creation.\\n\\n### 3. **Editing and Deleting Notes**\\nTo edit a note:\\n- Open the note you want to modify.\\n- Click on the text and make your changes.\\nTo delete a note:\\n- Click the three-dot menu next to the note title and select \\\"Delete.\\\"\\n\\n## Using AI-Powered Summarization\\n\\nNoterAI uses AI to help you quickly summarize your notes. Here\\u2019s how:\\n1. Open a note you\\u2019d like summarized.\\n2. Click on the **\\\"Summarize\\\"** button at the top of the note.\\n3. The AI will generate a brief summary based on the content of the note, which will appear at the bottom of the page.\\n\\nYou can save or modify the summary if needed.\\n\\n## Generating Quizzes\\n\\nNoterAI helps you create quizzes from your notes for better retention. To generate quizzes:\\n1. Select the note from which you want to create a quiz.\\n2. Click on the **\\\"Generate Quiz\\\"** button.\\n3. The AI will generate multiple-choice questions (MCQs) based on the content of your note.\\n4. You can take the quiz directly within the app, or you can save it for later.\\n\\nYou can also customize the quiz questions before starting the quiz.\\n\\n## Asking Questions (Q&A)\\n\\nIf you have specific questions based on your notes, NoterAI\\u2019s Q&A feature can provide instant answers:\\n1. Open a note or folder of notes.\\n2. Use the **\\\"Ask AI\\\"** button to type your question.\\n3. The AI will scan the relevant notes and provide an answer based on the content.\\n\\nThe Q&A feature is designed to help you clarify points or dive deeper into your study material.\\n\\n## Viewing Interesting Facts\\n\\nTo keep you motivated, NoterAI offers interesting facts related to your study topics:\\n1. After taking notes or summarizing content, you\\u2019ll occasionally see a pop-up or notification containing an interesting fact.\\n2. These facts are tailored to the subject or field you\\u2019re studying, helping you stay engaged and inspired.\\n\\nYou can also view a list of motivational and study-related facts in the \\\"Motivation\\\" section of the app.\\n\\n## Voice Input for Notes\\n\\nNoterAI allows you to create notes using your voice:\\n1. Click the **microphone icon** in the note editor.\\n2. Speak clearly, and the app will transcribe your speech into text in real-time.\\n3. Once done, review and edit the text as needed.\\n\\nThis feature is useful for quick note-taking during lectures or brainstorming sessions.\\n\\n## Accessing the App\\n\\n### Web\\nYou can access the NoterAI web app by visiting [NoterAI's web portal](https://noterai.vercel.app/) on any browser.\\n\\n### Mobile (iOS/Android)\\n- Download the **NoterAI** app from the [App Store](#) or [Google Play Store](#) (NOT PUBLISHED).\\n- Log in or sign up to start taking notes, generating quizzes, and using AI-powered features.\\n\\n### Desktop\\nFor desktop platforms (Windows, Linux, macOS), download the Flutter-based app from our [downloads page](#) (TODO).\\n\\n## Settings and Customizations\\n\\nYou can customize various aspects of the NoterAI experience:\\n- **Themes**: Switch between light, dark, or custom themes via the \\\"Settings\\\" menu.\\n- **Note Layouts**: Choose from different layouts for organizing and viewing your notes.\\n- **Notification Settings**: Enable or disable motivational facts and quiz reminders.\\n- **Account Settings**: Manage your personal details, password, and email notifications under \\\"Account.\\\"\\n\\n## Troubleshooting and Support\\n\\nIf you encounter any issues while using the app:\\n1. Visit our [FAQ section](./docs/FAQ.md) for common solutions.\\n2. If your issue persists, create an issue on our [GitHub repository](https://github.com/Himasnhu-AT/noterAI/issues/new).\\n\\nFor further assistance, contact us at **hyattherate2005 [at] gmail.com**.\\n\",\n              \"byteSize\": 4818\n            }\n          },\n          { \"name\": \"apps\", \"type\": \"tree\", \"mode\": 16384, \"object\": {} },\n          { \"name\": \"assets\", \"type\": \"tree\", \"mode\": 16384, \"object\": {} },\n          { \"name\": \"dev-docs\", \"type\": \"tree\", \"mode\": 16384, \"object\": {} },\n          { \"name\": \"docker\", \"type\": \"tree\", \"mode\": 16384, \"object\": {} },\n          {\n            \"name\": \"package.json\",\n            \"type\": \"blob\",\n            \"mode\": 33188,\n            \"object\": {\n              \"text\": \"{\\n  \\\"name\\\": \\\"@noterai/monorepo\\\",\\n  \\\"version\\\": \\\"0.0.1\\\",\\n  \\\"description\\\": \\\"Alternative of Notion, with cool features and AI functionality\\\",\\n  \\\"author\\\": {\\n    \\\"name\\\": \\\"Himanshu\\\",\\n    \\\"email\\\": \\\"hyattherate2005@gmail.com\\\",\\n    \\\"url\\\": \\\"https://github.com/Himasnhu-AT\\\"\\n  },\\n  \\\"main\\\": \\\"index.js\\\",\\n  \\\"scripts\\\": {\\n    \\\"pre-commit\\\": \\\"pnpm format && pnpm lint && pnpm build\\\",\\n    \\\"build\\\": \\\"turbo run build\\\",\\n    \\\"prod\\\": \\\"turbo run start\\\",\\n    \\\"dev\\\": \\\"turbo run dev\\\",\\n    \\\"backend:dev\\\": \\\"turbo run dev --filter=@noterai/backend\\\",\\n    \\\"backend:prod\\\": \\\"turbo run prod --filter=@noterai/backend\\\",\\n    \\\"backend:build\\\": \\\"turbo run build --filter=@noterai/backend\\\",\\n    \\\"frontend:dev\\\": \\\"turbo run dev --filter=@noterai/web\\\",\\n    \\\"frontend:prod\\\": \\\"turbo run prod --filter=@noterai/web\\\",\\n    \\\"frontend:build\\\": \\\"turbo run build --filter=@noterai/web\\\",\\n    \\\"docs:dev\\\": \\\"turbo run dev --filter=@noterai/docs\\\",\\n    \\\"docs:prod\\\": \\\"turbo run prod --filter=@noterai/docs\\\",\\n    \\\"docs:build\\\": \\\"turbo run build --filter=@noterai/docs\\\",\\n    \\\"lint\\\": \\\"turbo run lint\\\",\\n    \\\"format\\\": \\\"turbo run format\\\",\\n    \\\"test\\\": \\\"jest\\\"\\n  },\\n  \\\"workspaces\\\": [\\n    \\\"apps/*\\\",\\n    \\\"packages/*\\\",\\n    \\\"dev-docs\\\"\\n  ],\\n  \\\"repository\\\": {\\n    \\\"type\\\": \\\"git\\\",\\n    \\\"url\\\": \\\"git+https://github.com/Himasnhu-AT/noterAI.git\\\"\\n  },\\n  \\\"keywords\\\": [\\n    \\\"Editor\\\",\\n    \\\"Notion\\\",\\n    \\\"OpenSource\\\",\\n    \\\"notes\\\"\\n  ],\\n  \\\"packageManager\\\": \\\"pnpm@9.11.0\\\",\\n  \\\"license\\\": \\\"ISC\\\",\\n  \\\"bugs\\\": {\\n    \\\"url\\\": \\\"https://github.com/Himasnhu-AT/noterAI/issues\\\"\\n  },\\n  \\\"homepage\\\": \\\"https://github.com/Himasnhu-AT/noterAI#readme\\\",\\n  \\\"devDependencies\\\": {\\n    \\\"turbo\\\": \\\"^2.1.3\\\"\\n  }\\n}\\n\",\n              \"byteSize\": 1588\n            }\n          },\n          {\n            \"name\": \"pnpm-workspace.yaml\",\n            \"type\": \"blob\",\n            \"mode\": 33188,\n            \"object\": {\n              \"text\": \"packages:\\n  - \\\"apps/backend\\\"\\n  - \\\"apps/web\\\"\\n  - \\\"apps/docs\\\"\\n  - \\\"packages/*\\\"\\n  - \\\"dev-docs\\\"\\n\",\n              \"byteSize\": 92\n            }\n          },\n          {\n            \"name\": \"readme.md\",\n            \"type\": \"blob\",\n            \"mode\": 33188,\n            \"object\": {\n              \"text\": \"# NoterAI\\n\\n**NoterAI** helps students learn better by enabling them to take notes using a Notion-like UI or their voice. The platform leverages AI to summarize notes, generate quizzes, answer questions based on the notes, and keep students motivated with interesting facts related to their study topics.\\n\\n## Table of Contents\\n- [Features](#features)\\n- [Tech Stack](#tech-stack)\\n- [Installation Guide](#installation-guide)\\n- [Usage](#usage)\\n- [Contributing](#contributing)\\n- [License](#license)\\n- [Contact](#contact)\\n- [Acknowledgments](#acknowledgments)\\n\\n## Features\\n- **Note-Taking**: Create and organize notes using a Notion-like interface or via voice input.\\n- **Summarization**: Use AI to summarize your notes automatically.\\n- **Quizzes**: Generate quizzes from your notes to help with revision.\\n- **Q&A**: Ask questions and get answers based on your saved notes.\\n- **Strange Facts**: Stay motivated with interesting facts relevant to your study topics.\\n\\n## Tech Stack\\n- **Backend**: NestJS, Prisma, Redis\\n- **Mobile (Android, Windows, Linux)**: Flutter\\n- **iOS/macOS**: Swift\\n- **Web**: Next.js\\n- **AI**: NestJS (TypeScript) for main logic, Python for AI functionalities\\n\\n## Installation Guide\\nTo get NoterAI running locally, follow these steps:\\n\\n### Prerequisites\\nEnsure you have the following installed:\\n- **Node.js** (>= X.X.X)\\n- **npm** (>= X.X.X)\\n- **pnpm** (>= X.X.X)\\n- **Flutter** (>= X.X.X)\\n- **Docker** (for Redis and database setup)\\n\\n### Clone the Repository\\n```bash\\ngit clone https://github.com/himasnhu-at/noterai.git\\ncd noterai\\n```\\n\\n### Install Dependencies\\nUse `pnpm` to install all required dependencies:\\n```bash\\npnpm install\\n```\\n\\n### Set Up the Environment\\nCreate `.env` files for both the backend and web apps:\\n```bash\\ncp apps/backend/.env.example apps/backend/.env\\ncp apps/web/.env.example apps/web/.env\\n```\\nFill out the environment variables such as database URLs, Redis configurations, etc.\\n\\n### Start Development Environment\\nTo start the backend and frontend:\\n```bash\\npnpm dev\\n```\\nYou can also use Turbo for specific services:\\n```bash\\n# Backend\\nturbo run dev --filter=@noterai/backend\\n\\n# Web\\nturbo run dev --filter=@noterai/web\\n```\\n\\n### Running Flutter\\nFor mobile or desktop development:\\n```bash\\ncd apps/flutter\\nflutter run\\n```\\n\\nEnsure that you have a device or emulator running for Flutter.\\n\\n### Database Setup\\nStart the required services with Docker and set up the database:\\n```bash\\ndocker-compose up\\npnpm prisma migrate dev\\n```\\n\\nFor a more detailed setup guide, refer to our [Installation Guide](./INSTALLATION_GUIDE.md).\\n\\n## Usage\\nOnce the project is set up, you can access different features through the frontend or mobile app. Follow the [Usage Guide](./docs/usage.md) to learn how to utilize NoterAI's full range of functionalities, including note-taking, quiz generation, and AI-powered summaries.\\n\\n## Contributing\\nWe welcome contributions! Please review our [Contributing Guidelines](./CONTRIBUTING.md) before making any contributions. Make sure to also check out the [Code of Conduct](./CODE_OF_CONDUCT.md).\\n\\n### Quick Commands\\nHere are some common commands to help you contribute:\\n\\n- Start the development environment:\\n\\n```bash\\npnpm dev\\n```\\n\\n- Run tests:\\n\\n```bash\\npnpm test\\n```\\n\\n- Format and lint code:\\n\\n```bash\\npnpm format && pnpm lint\\n```\\n\\n## License\\nThis project is licensed under a **Custom License** that restricts commercial use. For more information, see the [LICENSE](./LICENSE) file.\\n\\n## Contact\\nIf you have any questions or need support, feel free to reach out at **hyattherate2005 [at] gmail.com**.\\n\\n## Acknowledgments\\nWe would like to thank all the contributors who helped in developing NoterAI. Special thanks to the AI community for their resources and support.\\n\\nEnjoy using NoterAI and enhance your learning experience!\\n\",\n              \"byteSize\": 3772\n            }\n          },\n          {\n            \"name\": \"setup.bat\",\n            \"type\": \"blob\",\n            \"mode\": 33188,\n            \"object\": {\n              \"text\": \"@echo off\\n\\nsetlocal\\n\\n:backend\\necho Running backend setup...\\ndocker-compose -f ./docker/docker-compose-backend.yml up -d\\n\\n:frontend\\necho Running frontend setup...\\ndocker-compose -f ./docker/docker-compose-frontend.yml up -d\\n\\n:db\\necho Running dB setup...\\ndocker-compose -f ./docker/docker-compose-dB.yml up -d\\n\\nif \\\"%1\\\"==\\\"backend\\\" (\\n    if exist .env.frontend (\\n        echo .env.frontend file found.\\n    ) else (\\n        echo .env.frontend file not found. Setting up environment...\\n        echo PORT=4000\\n        echo DATABASE_PORT=5432\\n        echo DATABASE_USERNAME=postgres\\n        echo DATABASE_PASSWORD=pass\\n        echo DATABASE_NAME=db\\n        echo DATABASE_URL=\\\"postgresql://postgres:pass@localhost:5432/db?schema=public\\\"\\n        echo REDDIS_URL=\\\"redis://localhost:6379\\\"\\n        echo JWT_SECRET='super-secret'\\n        echo JWT_EXPIRES_IN=604800\\n        echo EMAIL_ADDRESS=\\n        echo EMAIL_PASSWORD= # pass app-password if 2FA is enabled\\n        echo. > ./docker/.env.frontend\\n    )\\n    goto backend\\n)\\n\\nif \\\"%1\\\"==\\\"frontend\\\" (\\n    if exist .env.frontend (\\n        echo .env.frontend file found.\\n    ) else (\\n        echo .env.frontend file not found. Setting up environment...\\n        echo. > ./docker/.env.frontend\\n    )\\n    goto frontend\\n)\\n\\nif \\\"%1\\\"==\\\"db\\\" (\\n    goto db\\n)\\n\\nif \\\"%1\\\"==\\\"all\\\" (\\n    echo Running all setup...\\n    goto backend\\n    goto frontend\\n    goto db\\n)\\n\\necho ERROR:\\necho Please provide the correct argument.\\necho Example:\\necho.\\necho 1:   ./setup.bat backend    # setup backend\\necho 2:   ./setup.bat frontend   # setup frontend\\necho 3:   ./setup.bat db         # setup dB\\necho 4:   ./setup.bat all        # setup all\\n\\nendlocal\",\n              \"byteSize\": 1648\n            }\n          },\n          {\n            \"name\": \"setup.sh\",\n            \"type\": \"blob\",\n            \"mode\": 33261,\n            \"object\": {\n              \"text\": \"#!/bin/bash\\n\\nfun_backend() {\\n    echo \\\"Running backend setup...\\\"\\n    docker compose -f ./docker/docker-compose-backend.yml up -d\\n}\\n\\nfun_frontend() {\\n    echo \\\"Running frontend setup...\\\"\\n    docker compose -f ./docker/docker-compose-frontend.yml up -d    \\n}\\n\\nfun_db() {\\n    echo \\\"Running dB setup...\\\"\\n    docker compose -f ./docker/docker-compose-dB.yml up -d \\n}\\n\\nif [ \\\"$1\\\" == \\\"backend\\\" ]; then\\n    if [ -f .env.frontend ]; then\\n        echo \\\".env.frontend file found.\\\"\\n    else\\n        echo \\\".env.frontend file not found. Setting up environment...\\\"\\n        echo \\\"PORT=4000\\n\\nDATABASE_PORT=5432\\nDATABASE_USERNAME=postgres\\nDATABASE_PASSWORD=pass\\nDATABASE_NAME=db\\n\\nDATABASE_URL=\\\"postgresql://postgres:pass@localhost:5432/db?schema=public\\\"\\nREDDIS_URL=\\\"redis://localhost:6379\\\"\\n\\nJWT_SECRET='super-secret'\\nJWT_EXPIRES_IN=604800\\n\\nEMAIL_ADDRESS=\\nEMAIL_PASSWORD= # pass app-password if 2FA is enabled\\\" > ./docker/.env.frontend\\n    fi\\n    fun_backend\\n\\nelif [ \\\"$1\\\" == \\\"frontend\\\" ]; then\\n    if [ -f .env.frontend ]; then\\n        echo \\\".env.frontend file found.\\\"\\n    else\\n        echo \\\".env.frontend file not found. Setting up environment...\\\"\\n        echo \\\"\\\" > ./docker/.env.frontend\\n    fi\\n    fun_frontend\\n\\nelif [ \\\"$1\\\" == \\\"db\\\" ]; then\\n    fun_db\\n\\nelif [ \\\"$1\\\" == \\\"all\\\" ]; then\\n    echo \\\"Running all setup...\\\"\\n    fun_backend\\n    fun_frontend\\n    fun_db\\n\\nelse\\n    echo \\\"\\n    ERROR:\\n    Please provide the correct argument.\\n    Example:\\n    \\n    1:   ./setup.sh backend    # setup backend\\n    2:   ./setup.sh frontend   # setup frontend\\n    3:   ./setup.sh db         # setup dB\\n    4:   ./setup.sh all        # setup all\\n\\n    \\\"\\nfi\",\n              \"byteSize\": 1616\n            }\n          },\n          {\n            \"name\": \"turbo.json\",\n            \"type\": \"blob\",\n            \"mode\": 33188,\n            \"object\": {\n              \"text\": \"{\\n  \\\"$schema\\\": \\\"https://turbo.build/schema.json\\\",\\n  \\\"tasks\\\": {\\n    \\\"build\\\": {\\n      \\\"dependsOn\\\": [\\n        \\\"^build\\\",\\n        \\\"build:main\\\",\\n        \\\"build:ai-service\\\",\\n        \\\"build:collab_tools\\\",\\n        \\\"build:content_mgmt\\\",\\n        \\\"build:sharing_publishing_integration\\\"\\n      ],\\n      \\\"cache\\\": true\\n    },\\n    \\\"build:main\\\": {\\n      \\\"cache\\\": true\\n    },\\n    \\\"build:ai-service\\\": {\\n      \\\"cache\\\": true\\n    },\\n    \\\"build:collab_tools\\\": {\\n      \\\"cache\\\": true\\n    },\\n\\n    \\\"build:content_mgmt\\\": {\\n      \\\"cache\\\": true\\n    },\\n    \\\"build:sharing_publishing_integration\\\": {\\n      \\\"cache\\\": true\\n    },\\n    \\\"dev\\\": {\\n      \\\"dependsOn\\\": [\\n        \\\"^dev\\\",\\n        \\\"dev:main\\\",\\n        \\\"dev:ai-service\\\",\\n        \\\"dev:collab_tools\\\",\\n        \\\"dev:content_mgmt\\\",\\n        \\\"dev:sharing_publishing_integration\\\"\\n      ],\\n      \\\"cache\\\": true\\n    },\\n    \\\"dev:main\\\": {\\n      \\\"cache\\\": true\\n    },\\n    \\\"dev:ai-service\\\": {\\n      \\\"cache\\\": true\\n    },\\n    \\\"dev:collab_tools\\\": {\\n      \\\"cache\\\": true\\n    },\\n\\n    \\\"dev:content_mgmt\\\": {\\n      \\\"cache\\\": true\\n    },\\n    \\\"dev:sharing_publishing_integration\\\": {\\n      \\\"cache\\\": true\\n    },\\n    \\\"start\\\": {\\n      \\\"dependsOn\\\": [\\n        \\\"build\\\",\\n        \\\"^start\\\",\\n        \\\"prod:main\\\",\\n        \\\"prod:ai-service\\\",\\n        \\\"prod:collab_tools\\\",\\n        \\\"prod:content_mgmt\\\",\\n        \\\"prod:sharing_publishing_integration\\\"\\n      ],\\n      \\\"cache\\\": true\\n    },\\n    \\\"prod:main\\\": {\\n      \\\"cache\\\": true\\n    },\\n    \\\"prod:ai-service\\\": {\\n      \\\"cache\\\": true\\n    },\\n    \\\"prod:collab_tools\\\": {\\n      \\\"cache\\\": true\\n    },\\n\\n    \\\"prod:content_mgmt\\\": {\\n      \\\"cache\\\": true\\n    },\\n    \\\"prod:sharing_publishing_integration\\\": {\\n      \\\"cache\\\": true\\n    },\\n    \\\"test\\\": {\\n      \\\"dependsOn\\\": [\\\"^test\\\"],\\n      \\\"cache\\\": true\\n    },\\n    \\\"lint\\\": {\\n      \\\"dependsOn\\\": [\\\"^lint\\\"],\\n      \\\"cache\\\": true\\n    },\\n    \\\"format\\\": {\\n      \\\"dependsOn\\\": [\\\"^format\\\"],\\n      \\\"cache\\\": true\\n    }\\n  }\\n}\\n\",\n              \"byteSize\": 1861\n            }\n          }\n        ]\n      }\n    }\n  }\n}\n", "misc_bin/some.json": "{\"id\": 817395682, \"node_id\": \"R_kgDOMLh34g\", \"name\": \"noterAI\", \"full_name\": \"Himasnhu-AT/noterAI\", \"private\": false, \"owner\": {\"login\": \"Himasnhu-AT\", \"id\": 117301124, \"node_id\": \"U_kgDOBv3fhA\", \"avatar_url\": \"https://avatars.githubusercontent.com/u/117301124?v=4\", \"gravatar_id\": \"\", \"url\": \"https://api.github.com/users/Himasnhu-AT\", \"html_url\": \"https://github.com/Himasnhu-AT\", \"followers_url\": \"https://api.github.com/users/Himasnhu-AT/followers\", \"following_url\": \"https://api.github.com/users/Himasnhu-AT/following{/other_user}\", \"gists_url\": \"https://api.github.com/users/Himasnhu-AT/gists{/gist_id}\", \"starred_url\": \"https://api.github.com/users/Himasnhu-AT/starred{/owner}{/repo}\", \"subscriptions_url\": \"https://api.github.com/users/Himasnhu-AT/subscriptions\", \"organizations_url\": \"https://api.github.com/users/Himasnhu-AT/orgs\", \"repos_url\": \"https://api.github.com/users/Himasnhu-AT/repos\", \"events_url\": \"https://api.github.com/users/Himasnhu-AT/events{/privacy}\", \"received_events_url\": \"https://api.github.com/users/Himasnhu-AT/received_events\", \"type\": \"User\", \"user_view_type\": \"public\", \"site_admin\": false}, \"html_url\": \"https://github.com/Himasnhu-AT/noterAI\", \"description\": \"NoterAI is a product for students to help them study better and efficiently\", \"fork\": false, \"url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI\", \"forks_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/forks\", \"keys_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/keys{/key_id}\", \"collaborators_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/collaborators{/collaborator}\", \"teams_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/teams\", \"hooks_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/hooks\", \"issue_events_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/issues/events{/number}\", \"events_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/events\", \"assignees_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/assignees{/user}\", \"branches_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/branches{/branch}\", \"tags_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/tags\", \"blobs_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/git/blobs{/sha}\", \"git_tags_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/git/tags{/sha}\", \"git_refs_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/git/refs{/sha}\", \"trees_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/git/trees{/sha}\", \"statuses_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/statuses/{sha}\", \"languages_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/languages\", \"stargazers_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/stargazers\", \"contributors_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/contributors\", \"subscribers_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/subscribers\", \"subscription_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/subscription\", \"commits_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/commits{/sha}\", \"git_commits_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/git/commits{/sha}\", \"comments_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/comments{/number}\", \"issue_comment_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/issues/comments{/number}\", \"contents_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/contents/{+path}\", \"compare_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/compare/{base}...{head}\", \"merges_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/merges\", \"archive_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/{archive_format}{/ref}\", \"downloads_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/downloads\", \"issues_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/issues{/number}\", \"pulls_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/pulls{/number}\", \"milestones_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/milestones{/number}\", \"notifications_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/notifications{?since,all,participating}\", \"labels_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/labels{/name}\", \"releases_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/releases{/id}\", \"deployments_url\": \"https://api.github.com/repos/Himasnhu-AT/noterAI/deployments\", \"created_at\": \"2024-06-19T16:01:47Z\", \"updated_at\": \"2024-12-26T14:22:05Z\", \"pushed_at\": \"2025-01-04T01:11:42Z\", \"git_url\": \"git://github.com/Himasnhu-AT/noterAI.git\", \"ssh_url\": \"git@github.com:Himasnhu-AT/noterAI.git\", \"clone_url\": \"https://github.com/Himasnhu-AT/noterAI.git\", \"svn_url\": \"https://github.com/Himasnhu-AT/noterAI\", \"homepage\": \"https://noterai.vercel.app\", \"size\": 3709, \"stargazers_count\": 6, \"watchers_count\": 6, \"language\": \"TypeScript\", \"has_issues\": true, \"has_projects\": true, \"has_downloads\": true, \"has_wiki\": true, \"has_pages\": false, \"has_discussions\": false, \"forks_count\": 8, \"mirror_url\": null, \"archived\": false, \"disabled\": false, \"open_issues_count\": 7, \"license\": {\"key\": \"other\", \"name\": \"Other\", \"spdx_id\": \"NOASSERTION\", \"url\": null, \"node_id\": \"MDc6TGljZW5zZTA=\"}, \"allow_forking\": true, \"is_template\": false, \"web_commit_signoff_required\": false, \"topics\": [\"hacktoberfest\", \"hacktoberfest2024\", \"nestjs-backend\", \"nextjs\", \"notes\", \"notion\", \"notion-alternative\", \"opensource\"], \"visibility\": \"public\", \"forks\": 8, \"open_issues\": 7, \"watchers\": 6, \"default_branch\": \"main\", \"permissions\": {\"admin\": false, \"maintain\": false, \"push\": false, \"triage\": false, \"pull\": true}, \"temp_clone_token\": \"\", \"network_count\": 8, \"subscribers_count\": 1}", "results/basic_details.txt": "Repository Details:\n  Description: Google's Search alternative written in rust for creation and usage as a private search engine\n  Stars: 2\n  Forks: 0\n  Open Issues: 8\n  Watchers: 2\n\nBranches:\n  Himasnhu-AT-patch-1\n  dependabot/npm_and_yarn/dev-docs/rollup-4.22.4\n  dependabot/npm_and_yarn/dev-docs/vite-5.4.6\n  main\n", "results/dependencies.txt": "Based on the provided Rust code, here are some of the libraries and frameworks that appear to be required:\n\n**Required Libraries:**\n\n1. **wasm32-unknown-unknown**: This is a target for WebAssembly compilation, which allows the code to run in web browsers.\n2. **serde**: This library provides serialization and deserialization functionality for Rust data structures.\n3. **serde_json**: This library provides JSON serialization and deserialization functionality for Rust data structures.\n4. **serde_yaml**: This library provides YAML serialization and deserialization functionality for Rust data structures.\n5. **std::collections`: This is the standard library's collection module, which provides various data structures such as hash maps, sets, and queues.\n\n**Required Frameworks:**\n\n1. **No framework appears to be explicitly required**, but it can be inferred that the code uses the following:\n\t* **async/await**: The use of `async` and `await` keywords suggests that the code is using async programming.\n\t* **tokio**: Although not explicitly mentioned, the use of async/await and WebAssembly compilation suggests that Tokio (a Rust framework for building asynchronous web applications) might be used.\n\n**Additional Libraries and Frameworks:**\n\n1. **tf-idf calculation library**: The code appears to be using a TF-IDF calculation library, but the specific library is not mentioned.\n2. **Data storage library**: The code uses WebAssembly buffers to store data, suggesting that a data storage library such as `wasm32-unknown-unknown` might be used.\n\n**Possible Dependencies:**\n\n1. ** Cargo.toml**: The file contains dependencies declared in the `Cargo.toml` file, which is not shown here.\n2. **Other libraries and frameworks**: Depending on the specific requirements of the project, other libraries and frameworks might be required.\n\nPlease note that this analysis is based on the provided code snippet and might not cover all possible dependencies or libraries used in the full codebase.", "results/file_types.json": "{\n  \"gitignore\": 7.6923076923076925,\n  \"gitmodules\": 2.564102564102564,\n  \"LICENSE\": 2.564102564102564,\n  \"md\": 33.33333333333333,\n  \"js\": 5.128205128205128,\n  \"json\": 5.128205128205128,\n  \"yaml\": 2.564102564102564,\n  \"lock\": 2.564102564102564,\n  \"toml\": 2.564102564102564,\n  \"xhtml\": 10.256410256410255,\n  \"rs\": 23.076923076923077,\n  \"html\": 2.564102564102564\n}\n", "results/final_summaries.json": "{\".gitignore\": \"I'm ready to help, but you didn't provide the contents of the .gitignore file. Please share it with me, and I'll be happy to summarize it in 30 words.\", \".gitmodules\": \"The file lists Git submodules for 'snowball' and 'engine/docs.gl' projects, referencing GitHub repositories with specific URLs and paths.\", \"LICENSE\": \"Permission to use SYNTHAI-LABS OpenSource Software is granted, with restrictions on commercial use and requiring copyright notice in all copies or substantial portions.\", \"Readme.md\": \"FastSearch is a Rust implementation of Google's search engine with caching and plans for multi-threading, benchmarking, and client installation. Stable release planned by mid-June, work in progress.\", \"dev-docs/.gitignore\": \"dev-docs/.gitignore:\\nnode_modules/ .vitepress/cache/ .vitepress/dist/\", \"dev-docs/.vitepress/config.js\": \"VitePress configuration for a private search engine website with navigation and sidebar layouts, including links to GitHub repository and various documentation pages.\", \"dev-docs/changelog.md\": \"Version updates include txt file support, speed improvement, caching, subcommands, tf-idf algorithm implementation, and repository setup.\", \"dev-docs/code-explanation.md\": \"Overview of code structure and explanations in FastSearch project's code documentation.\", \"dev-docs/code-structure.md\": \"The code structure consists of `/packages/engine`, containing main.rs, docs.gl, small, and webclient subdirectories, with external dependencies including serde_json, tiny_http, and xml-rs.\", \"dev-docs/contributing.md\": \"Contribute guidelines emphasize transparency and ease of use for reporting bugs, discussing code state, submitting fixes, proposing features, or becoming a maintainer, all hosted on GitHub.\", \"dev-docs/index.md\": \"FastSearch Developer Documentation provides resources for developers to use and contribute to the project, covering topics like Getting Started, Contributing, Code Explanation, and more.\", \"dev-docs/license.md\": \"SYNTHAI-LABS OpenSource License grants free use, modify, and distribute rights to the software, but prohibits commercial use with no warranty or liability for any claims.\", \"dev-docs/package.json\": \"\\\"FastSearch Developer Documentation\\\" with VitePress as dev dependency, version 1.0.0, and scripts for development, build, and testing environments.\", \"dev-docs/pnpm-lock.yaml\": \"The provided output is a summary of the packages and their dependencies in a Node.js project. Here's a breakdown of what each part means:\\n\\n1. **Package**: A package is a dependency that is installed using npm or yarn.\\n2. **Version**: The version number of the package, which indicates its current state (e.g., 4.17.2 for Rollup).\\n3. **Integrity**: A hash value that verifies the package's authenticity and ensures it hasn't been tampered with during installation.\\n4. **Peer Dependencies**: Other packages that are dependent on the current package, but aren't required for its own functionality.\\n5. **Transitive Peer Dependencies**: Peer dependencies that inherit their requirements from a package (e.g., `@vue/composition-api` is a transitive peer dependency of `vue-demi`).\\n6. **Optional Dependencies**: Packages that are not strictly required by the project, but can be installed if needed (e.g., `fsevents: 2.3.3`).\\n\\nHere are some observations based on this output:\\n\\n* The project uses many Vue-related packages, which suggests it's a Vue.js application.\\n* There are several dependencies from Algolia, indicating that the project uses Algolia's search service.\\n* The presence of `esbuild`, `postcss`, and `rollup` suggests that the project is using these tools for bundling and transpilation.\\n* Some packages have high version numbers (e.g., `8.4.38` for PostCSS), indicating that they might be used in a legacy or older application.\\n\\nOverall, this summary provides a good overview of the dependencies used in the project, but doesn't reveal anything new about the project's specific functionality or features.\", \"dev-docs/readme.md\": \"FastSearch Dev Docs is a Rust-based implementation of Google's Search Engines with privatization features. Install via Git and run using cargo, or build using Docker.\", \"dev-docs/tf_idf.md\": \"TF-IDF is a numerical statistic reflecting word importance in documents, calculated as TF(t) * IDF(t), with term frequency ( appearances / total terms) and inverse document frequency (log(documents with term / total documents)).\", \"dev-docs/todo.md\": \"Development tasks: Release v1.0 to improve speed and UI, then add features, followed by release v1.1 with database integration and testing, concluding with v1.2 enhancements and further testing.\", \"packages/engine/.gitignore\": \"*.pyc\\n__pycache__\\n*.egg-info\\n*.whl\\ndist/\\nbuild/\\n*.zip\\n*.tar.gz\\n*.egg-info/\", \"packages/engine/Cargo.lock\": \"Rust package dependencies for engine 0.0.3, including ascii, cc, chunked_transfer, httpdate, itoa, libc, log, pkg-config, proc-macro2, quote, ryu, serde, sqlite, tiny_http, and xml-rs versions 1.1.0 to 0.8.20.\", \"packages/engine/Cargo.toml\": \"The Cargo.toml file lists the Rust package \\\"engine\\\" with version 0.0.3, dependencies including serde and sqlite, and specifies their respective versions and features.\", \"packages/engine/index-small.json\": \"The provided output appears to be a JSON-like data dump, containing key-value pairs and various metadata. It's difficult to determine the exact purpose or context of this data without more information.\\n\\nHowever, I can attempt to provide some general insights:\\n\\n1. **GLSL shader source code**: The output contains several lines starting with `#version`, which suggests that it might be a GLSL (OpenGL Shading Language) shader source code.\\n2. **Key-value pairs**: The data dump consists of key-value pairs, where each key is typically a string and the value is also a string or an integer. Some keys seem to represent constants, variables, or function calls.\\n3. **Shader parameters and uniforms**: Many of the key-value pairs appear to be related to shader parameters and uniforms, such as `gl_FragCoord`, `gl_Position`, and `gl_ModelViewMatrix`.\\n4. **Functions and macros**: There are references to functions like `glGetTexImage` and `glCopyTexSubImage2D`, which suggests that the code is interacting with OpenGL functionality.\\n5. **Numerical values**: Some values appear to be numerical, such as `64` or `1/3`.\\n\\nWithout more context or information about what this data represents, it's challenging to provide a more specific summary. If you have any additional details or can clarify the purpose of this data dump, I'd be happy to try and assist further!\", \"packages/engine/small/glAttachShader.xhtml\": \"This appears to be a documentation page for the OpenGL API, specifically for the `glAttachShader` function. Here is a summary of the content:\\n\\n**Function Description**\\n\\nThe `glAttachShader` function is used to attach a shader object to an OpenGL program.\\n\\n**Parameters**\\n\\n* The first parameter is the shader object to be attached.\\n* The second parameter is the program object to which the shader will be attached.\\n\\n**Return Value**\\n\\nNo return value is specified, but it is implied that the function will return an error code if the attachment fails.\\n\\n**Notes**\\n\\nThe table shows the compatibility of `glAttachShader` with different OpenGL versions and implementations. It indicates that `glAttachShader` has been supported since version 2.0 and is compatible with most OpenGL implementations.\\n\\n**See Also**\\n\\nThe page also mentions other related functions, such as `glCompileShader`, `glCreateShader`, `glDeleteShader`, `glDetachShader`, `glLinkProgram`, and `glShaderSource`.\\n\\n**Copyright Information**\\n\\nFinally, the page notes that the material is copyrighted by 3Dlabs Inc. Ltd. and the Khronos Group, and can be distributed under the terms of the Open Publication License, v 1.0.\", \"packages/engine/small/glBeginConditionalRender.xhtml\": \"This is a summary of the OpenGL documentation for the `glBeginConditionalRender` function.\\n\\n**Function Overview**\\n\\nThe `glBeginConditionalRender` function is used to begin rendering an arbitrary number of client-side buffers. It allows a program to render different parts of its display independently, and is particularly useful for debugging and optimization purposes.\\n\\n**Parameters**\\n\\n*   The first parameter is the buffer object that should be used as a target.\\n*   The second parameter specifies which type of conditional rendering mode to use (e.g., `GL_FALSE` or `GL_TRUE`).\\n*   The third parameter specifies which part of the display should be rendered (a range of pixels in 2D or a range of pixels in 3D).\\n\\n**Return Value**\\n\\nThe function returns an OpenGL error code.\\n\\n**See Also**\\n\\n*   `glEndConditionalRender`\\n*   `glGenQueries`\\n*   `glDeleteQueries`\\n\\n**Copyright Information**\\n\\nThis documentation is copyright by the Khronos Group, and can be distributed under the Open Publication License.\", \"packages/engine/small/glCompressedTexImage1D.xhtml\": \"The provided text appears to be a part of the OpenGL documentation, specifically the section describing the `glCompressedTexImage` functions. The main topic is about compressed texture images and the functions used to work with them.\\n\\nKey points mentioned include:\\n\\n* Compressed texture images are stored in row-major (Fortran) order.\\n* The `glCompressedTexImage` functions store a compressed texture image into the first `level * level` pixels of the buffer.\\n* The `glCompressedTexSubImage` functions subimage into a specific portion of an already-compressed texture image.\\n* The compression formats used are DXT1, ETC1, and S3TC (S3 Texture Compression).\\n\\nThe text also mentions related OpenGL functions for working with texture images, such as `glCopyTexImage`, `glCopyTexSubImage`, and `glPixelStore`.\\n\\nAdditionally, the document provides a list of \\\"See Also\\\" references to other relevant OpenGL functions and topics.\\n\\nLastly, it includes copyright information, stating that the document is licensed under the SGI Free Software B License.\", \"packages/engine/small/glCopyTexImage1D.xhtml\": \"This is a documentation page for the OpenGL API, specifically the `glCopyTexImage1D` function. Here's a summary of what I can gather:\\n\\n**Function Overview**\\n\\n`glCopyTexImage1D` copies image data from one buffer to another.\\n\\n**Parameters**\\n\\n* `target`: The target to copy the image to (e.g., texture units)\\n* `level`: The level of detail for the image\\n* `format`: The format of the image data\\n* `type`: The type of the image data (e.g., unsigned byte, signed integer)\\n* `x`, `y`, and `width`/`height`: The coordinates and dimensions of the region to copy\\n\\n**Return Value**\\n\\nNone. This function modifies the target buffer.\\n\\n**See Also**\\n\\nOther OpenGL functions that are related or similar to `glCopyTexImage1D`, such as:\\n\\n* `glCopyTexImage2D`\\n* `glCopyTexSubImage1D`\\n* `glCopyTexSubImage2D`\\n* `glPixelStore`\\n* `glTexImage1D`\\n* `glTexImage2D`\\n* `glTexSubImage1D`\\n* `glTexSubImage2D`\\n* `glTexParameter`\\n\\n**License**\\n\\nThis documentation is licensed under the SGI Free Software B License, which allows for free use and modification of the software.\", \"packages/engine/src/lexer.rs\": \"A Rust lexer struct `Lexer` processes character arrays, trims whitespace and chops tokens based on character properties.\", \"packages/engine/src/main.rs\": \"This is not a code review, but rather an extraction of information from the provided text. However, I can provide some general observations and suggestions.\\n\\n**General Observations**\\n\\n1. The text appears to be a documentation or README file for a project, possibly a web search engine.\\n2. It provides information about the project's functionality, including indexing, searching, and serving.\\n3. There are references to database modes (e.g., SQLite) and file formats (e.g., JSON).\\n\\n**Suggestions**\\n\\n1. **Organization**: The text could benefit from being reorganized into clear sections or categories, such as \\\"Introduction,\\\" \\\"Functionality,\\\" and \\\"Technical Details.\\\"\\n2. **Consistency**: Some sentences are written in a imperative tone (\\\"use_sqlite_mode = true\\\"), while others are more descriptive (\\\"the project uses SQLite mode by default\\\").\\n3. **Style**: The text could benefit from a consistent writing style, such as using standard punctuation or formatting conventions.\\n4. **Clarity**: Some phrases, like \\\"TODO: search result must consist of clickable links,\\\" are cryptic and unclear without additional context.\\n\\n**Extracted Code Snippets**\\n\\nThe provided text does not contain any actual code snippets. However, there are references to code, such as:\\n\\n1. `SqliteModel::open(Path::new(&index_path))?`\\n2. `serde_json::from_reader(index_file)?`\\n3. `server::start(&address, &model)`\\n\\nThese lines suggest that the project uses Rust programming language and depends on external libraries like SQLite and JSON serialization.\\n\\nPlease let me know if you'd like me to help with anything else!\", \"packages/engine/src/model.rs\": \"Sqlite database model for search engine with InMemoryModel and SqliteModel implementing Model trait.\", \"packages/engine/src/server.rs\": \"Serves HTTP requests with various responses for 404, 500, and static files, as well as handles API search queries.\", \"packages/engine/src/snowball/algorithms/english_stemmer.rs\": \"This is a Rust code snippet that implements the stem algorithm for stemming words in natural language processing (NLP). The algorithm appears to be designed for Snowball library.\\n\\nThe `stem` function is the main entry point, which takes a mutable reference to a `SnowballEnv` object as an argument. It initializes a new context and enters a loop where it checks if there are any exceptions (likely related to word boundaries or punctuation) using the `r_exception1` function.\\n\\nAfter that, it jumps back to the beginning of the text using the `hop` method with an offset of 3, and then calls the `prelude`, `mark_regions`, and other functions in sequence. These functions seem to perform various pre-processing steps on the input text.\\n\\nThe loop continues until all words are processed, at which point it breaks out of the loop and returns true.\\n\\nHere's a high-level overview of the code:\\n\\n1. Initialize a new context with default values.\\n2. Enter a loop where:\\n\\t* Check if there are any exceptions using `r_exception1`.\\n\\t* Jump back to the beginning of the text using `hop` with an offset of 3.\\n\\t* Call pre-processing functions in sequence (`prelude`, `mark_regions`, etc.).\\n3. Break out of the loop when all words are processed, and return true.\\n\\nNote that this code snippet is incomplete, as it only shows a portion of the algorithm implementation. The full implementation would require more context and additional information about the specific requirements of the Snowball library.\", \"packages/engine/src/snowball/algorithms/mod.rs\": \"The Snowball library includes a module for English language stemming, with another module referred to in the build script but not included here.\", \"packages/engine/src/snowball/among.rs\": \"The Among struct in Rust contains a string, two integers, and an optional closure that takes a mutable reference to a SnowballEnv and a mutable reference to any type T.\", \"packages/engine/src/snowball/mod.rs\": \"The Snowball Rust library has modules for algorithms, 'among' and 'snowball_env', with public usage of Among and SnowballEnv. TODO comments are present regarding license and using crate.\", \"packages/engine/src/snowball/snowball_env.rs\": \"This is a Rust implementation of an editor with various methods for manipulating text, inserting and deleting characters, searching for patterns, and more.\\n\\nHere are some key features and observations:\\n\\n**Editor State**\\n\\nThe editor maintains two main states: `cursor` and `limit`. The `cursor` represents the current position in the text, while the `limit` is the end of the buffer.\\n\\n**Methods**\\n\\nSome notable methods include:\\n\\n* `find_among`: Searches for a pattern in the text by comparing it to an array of \\\"amongs\\\". It returns the index of the first match.\\n* `find_among_b`: Similar to `find_among`, but searches backwards from the current position.\\n* `slice_to` and `slice_from`: Returns or sets a slice of characters within the buffer.\\n\\n**Comparison Methods**\\n\\nMethods like `compare_to` (not shown) are used to compare two buffers byte-wise, which is useful for searching patterns. These methods are likely implemented recursively to traverse the buffers efficiently.\\n\\n**Amongs**\\n\\nAn \\\"among\\\" is an array of bytes that represents a pattern to search for in the text. Each among has three fields: `0` (the field name), `1` (a boolean flag), and `2` (a boolean flag). The first flag indicates whether this among should be considered as a prefix or suffix, while the second flag determines the direction of comparison.\\n\\n**Text Comparison**\\n\\nThe implementation uses a simple byte-wise comparison to search for patterns. However, some methods like `compare_to` may use more advanced techniques, such as using a trie data structure to efficiently store and compare prefixes.\\n\\n**Editor Interface**\\n\\nWhile this code is not an editor itself, it appears to be part of a larger system that provides an interface for interacting with the buffer. The interface would likely include methods for inserting, deleting, or modifying characters in the buffer.\\n\\n**Notes**\\n\\n* The code uses a mix of Rust's advanced features (e.g., iterators, closures) and more traditional techniques (e.g., loops).\\n* Some methods are not implemented, suggesting that this is an incomplete implementation.\\n* There may be opportunities to improve performance by using more efficient algorithms or data structures.\", \"packages/engine/webclient/index.html\": \"This HTML document creates a simple private search engine webpage with a title, input field for searching, and a container to display results.\", \"packages/engine/webclient/index.js\": \"Async function searches for a keyword in the \\\"results\\\" div, sending a POST request to /api/search with the query.\", \"performance.md\": \"Performance data for `docs.gl` folder, including user and system times, CPU usage, and total times, with variations between debug and release builds.\", \"todo_add_docs.md\": \"The file outlines a format for an index with two main sections: 'df' and 'tfpd', used to store data on document frequency and term frequency.\", \"working-logs.md\": \"\\\"Optimized engine serving logs, with performance metrics for 'bind texture to buffer' and 'bind, to buffer' queries, showing varying execution times across different OpenGL versions.\\\"\"}", "results/full_project.txt": "The provided text appears to be a collection of files related to an indexing system for a WebGL rendering engine, possibly a variant of the OpenGL ES API. The files seem to contain logs and documentation related to the system's performance, functionality, and usage.\n\nHere is a summary of the key points from the files:\n\n1. **Indexing System**: The system appears to be designed to index and store data about WebGL textures, buffers, and other resources. It uses a JSON-based format for storing metadata.\n2. **Performance Metrics**: The logs contain performance metrics for various operations, such as querying the system's cache hit rates or execution times for specific queries.\n3. **Cache Hit Rates**: The logs show that the system has significant cache hit rates for certain operations, indicating efficient data storage and retrieval mechanisms.\n4. **Query Optimization**: The logs suggest that the system uses query optimization techniques to reduce execution time for frequent queries.\n5. **System Variants**: The logs mention different variants of the system, such as \"debug\" and \"release\" modes, which likely correspond to different levels of optimization or logging verbosity.\n\nSome specific points from the logs include:\n\n* A 19ms serve time after caching TF calculation (Release mode)\n* A 140ms serve time after caching IDf calculation (Debug mode)\n* Query execution times for various operations, such as \"bind texture to buffer\" and \"bind, to buffer\"\n* Execution times for system variants, including Release and Debug modes\n\nThe files also contain documentation on the format of the index data, which is represented in JSON format. The documentation explains that the index contains two main sections: \"df\" (document frequency) and \"tfpd\" (term-frequency per document).\n\nOverall, the text provides insights into the performance, functionality, and usage of an indexing system for a WebGL rendering engine.", "results/tree_struct.txt": "|-- .gitignore\n|-- .gitmodules\n|-- LICENSE\n|-- Readme.md\n|-- assets\n    |-- demo.png\n    |-- workingmodel.mov\n|-- dev-docs\n    |-- .gitignore\n    |-- .vitepress\n        |-- config.js\n    |-- changelog.md\n    |-- code-explanation.md\n    |-- code-structure.md\n    |-- contributing.md\n    |-- index.md\n    |-- license.md\n    |-- package.json\n    |-- pnpm-lock.yaml\n    |-- readme.md\n    |-- tf_idf.md\n    |-- todo.md\n|-- packages\n    |-- engine\n        |-- .gitignore\n        |-- Cargo.lock\n        |-- Cargo.toml\n        |-- docs.gl\n        |-- index-big.json\n        |-- index-small.json\n        |-- index.json\n        |-- small\n            |-- glAttachShader.xhtml\n            |-- glBeginConditionalRender.xhtml\n            |-- glClearBufferData.xhtml\n            |-- glCompressedTexImage1D.xhtml\n            |-- glCopyTexImage1D.xhtml\n        |-- src\n            |-- lexer.rs\n            |-- main.rs\n            |-- model.rs\n            |-- server.rs\n            |-- snowball\n                |-- algorithms\n                    |-- english_stemmer.rs\n                    |-- mod.rs\n                |-- among.rs\n                |-- mod.rs\n                |-- snowball_env.rs\n        |-- webclient\n            |-- index.html\n            |-- index.js\n|-- performance.md\n|-- todo_add_docs.md\n|-- working-logs.md", "run_back_files.sh": "# #!bin/zsh\n\n# this is a shebang this tell the comipler whuich interpreter to use\n\necho \"Starting get_file_dat_API.py...\"\npython get_file_dat_API.py\n\necho \"Starting try_summarize.py...\"\npython try_summarize.py\n\necho \"Starting get_file_types.py...\"\npython get_file_types.py\n\necho \"All scripts executed.\"\n\n# to run all commands in parallel we can use command1 & command2 & command3, and && I think for sequential thing\n# chmod +x run_all.sh, to make it executable\n", "temp/check.txt": ".gitignore: doing\n.gitmodules: doing\nLICENSE: doing\nReadme.md: doing\ndev-docs/.gitignore: doing\ndev-docs/.vitepress/config.js: doing\ndev-docs/changelog.md: doing\ndev-docs/code-explanation.md: doing\ndev-docs/code-structure.md: doing\ndev-docs/contributing.md: doing\ndev-docs/index.md: doing\ndev-docs/license.md: doing\ndev-docs/package.json: doing\ndev-docs/pnpm-lock.yaml: doing\ndev-docs/readme.md: doing\ndev-docs/tf_idf.md: doing\ndev-docs/todo.md: doing\npackages/engine/.gitignore: doing\npackages/engine/Cargo.lock: doing\npackages/engine/Cargo.toml: doing\npackages/engine/index-small.json: doing\npackages/engine/small/glAttachShader.xhtml: doing\npackages/engine/small/glBeginConditionalRender.xhtml: doing\npackages/engine/small/glCompressedTexImage1D.xhtml: doing\npackages/engine/small/glCopyTexImage1D.xhtml: doing\npackages/engine/src/lexer.rs: doing\npackages/engine/src/main.rs: doing\npackages/engine/src/model.rs: doing\npackages/engine/src/server.rs: doing\npackages/engine/src/snowball/algorithms/english_stemmer.rs: doing\npackages/engine/src/snowball/algorithms/mod.rs: doing\npackages/engine/src/snowball/among.rs: doing\npackages/engine/src/snowball/mod.rs: doing\npackages/engine/src/snowball/snowball_env.rs: doing\npackages/engine/webclient/index.html: doing\npackages/engine/webclient/index.js: doing\nperformance.md: doing\ntodo_add_docs.md: doing\nworking-logs.md: doing\n", "todo.md": "GitHub summarizer\n\n\nTo-Do major topics\n\n1. Make a directory tree- Done\n2. Make the summarizer for each file - Done\n3. Make the actual complete summarizer - Done\n4. Make the Language file thing similar to the GitHub - ToDo\n5. Make an Algo clone similar to the github next - ToDo\n6. Use rust to make documentation for the entire project\n\nTo do sanskar:\nhttps://gitingest.com/\n\n1. html, css to form template\n2. js dom feature\n3. api calls // use weather api make a weather app\n\n--> GraphQL in Python then TS\n--> Flask\n-->\n\nMinor Topics\n\n1. Github call -> json\n2. org_json -> filesystem\n3. api endpoint -> org_json (api calls)\n3. Panda: filesystem(filesystem_json) -> render html\n4. json(filesystem_json -> api calls(api_json) -> render(HTML))\n2.\n\n\nDevansh working:\n\n1. Code Analysis Languages -- Done\n2. File Structure -- Done\n3. Dependencies -- Done\n4. Repo Info -- Done\n5. Config -- Postponed\n6. Add context thing to each file summary\n", "try_summarize.py": "import ollama\nimport json\nimport os\n\n# Load JSON data\nwith open('results/files_data.json', 'r') as file:\n    data = json.load(file)\n\n# Initialize the Ollama client\nclient = ollama.Client()\n\nconversation_history = [\n\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n\n    {\"role\": \"assistant\", \"content\": \"Paris\"}\n\n]\n\ndirectory = \"temp\"\nif not os.path.exists(directory):\n    os.mkdir(directory)\nwith open(\"temp/check.txt\", 'w') as f:\n    pass\n\n\ndef summarize_file(filename, content):\n    with open(\"temp/check.txt\", 'a') as f:\n        f.write(f\"{filename}: doing\\n\")\n    prompt = f\"Summarize the following file, in 30 words and only include the actual content don't say that here is a file in 30 words I just need the actual data summary: {filename}:\\n\\n{content}\\n\\nSummary:\"\n    response = client.generate(model='llama3.2:latest', prompt=prompt)\n    summary =  response['response'].strip()\n\n    return summary\n\ndef summarize_the_entire_thing(content):\n    content = str(content)\n    prompt = f\"Summarize this entire project in detail(don't give unnecessary filler words directly give the summary don't give things like: This is a JSON file): {content}:\\n\\nSummary\"\n    response = client.generate(model='llama3.2:latest', prompt=prompt)\n    summary =  response['response'].strip()\n\n    return summary\n\n\ndef get_dependencies(content):\n    # Start by creating the initial conversation with a system message and the first user prompt\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an assistant that specializes in analyzing code projects and providing precise library and framework dependencies.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Tell me what are the libraries and frameworks required for this project given in the JSON file: {content}\"\n        }\n    ]\n\n    # Get the first response based on the first prompt\n    response1 = client.chat(model='llama3.2:latest', messages=messages)\n    # Optionally, capture the assistant's reply\n    answer1 = response1['message']['content']\n    # print(\"First response:\", answer1)\n\n    # Append the assistant's answer to the conversation history to provide context\n    messages.append({\n        \"role\": \"assistant\",\n        \"content\": answer1\n    })\n\n    # Add the second user prompt, which will now include context from the first interaction\n    messages.append({\n        \"role\": \"user\",\n        \"content\": \"tell me the libraries and frameworks required for this code\"\n    })\n\n    # Get the response; the conversation history now contains both prompts to guide the model\n    response2 = client.chat(model='llama3.2:latest', messages=messages)\n    dependencies = response2['message']['content'].strip()\n    print(\"Final dependencies:\", dependencies)\n\n    return dependencies\n\n\nfile_summaries = {}\nfor filename, content in data.items():\n    file_summaries[filename] = summarize_file(filename, content)\n\n# Save summaries to JSON file\nwith open('results/final_summaries.json', 'w') as f:\n    json.dump(file_summaries, f)\n\nwith open('results/full_project.txt', 'w') as f:\n    f.write(summarize_the_entire_thing(data))\n\nwith open('results/dependencies.txt', 'w') as f:\n    f.write(get_dependencies(data))\n# Print the summaries\nfor filename, summary in file_summaries.items():\n    print(f\"File: {filename}\\nSummary: {summary}\\n\")\n"}